{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f7cc81",
   "metadata": {},
   "source": [
    "### a) data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e0fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_text = \"x_train.txt\"\n",
    "y_text = \"y_train.txt\"\n",
    "\n",
    "x_clients = \"x_test.txt\"\n",
    "\n",
    "df_variables = pd.read_csv(\n",
    "    x_text,\n",
    "    delimiter=\" \",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "df_labels = pd.read_csv(\n",
    "    y_text,\n",
    "    delimiter=\" \",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "df_variables_clients  = pd.read_csv(\n",
    "    x_clients,\n",
    "    delimiter=\" \",\n",
    "    header=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9477df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.454647</td>\n",
       "      <td>16.739345</td>\n",
       "      <td>39.811892</td>\n",
       "      <td>24.955468</td>\n",
       "      <td>27.088535</td>\n",
       "      <td>17.116793</td>\n",
       "      <td>25.166957</td>\n",
       "      <td>23.364508</td>\n",
       "      <td>17.223886</td>\n",
       "      <td>18.339175</td>\n",
       "      <td>...</td>\n",
       "      <td>13.235314</td>\n",
       "      <td>5.160379</td>\n",
       "      <td>29.194846</td>\n",
       "      <td>17.298314</td>\n",
       "      <td>6.414267</td>\n",
       "      <td>7.780568</td>\n",
       "      <td>6.840910</td>\n",
       "      <td>18.295197</td>\n",
       "      <td>10.014028</td>\n",
       "      <td>6.938318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.175225</td>\n",
       "      <td>10.483281</td>\n",
       "      <td>27.471017</td>\n",
       "      <td>18.509824</td>\n",
       "      <td>19.045353</td>\n",
       "      <td>15.039082</td>\n",
       "      <td>21.354915</td>\n",
       "      <td>15.790575</td>\n",
       "      <td>13.912508</td>\n",
       "      <td>13.772518</td>\n",
       "      <td>...</td>\n",
       "      <td>13.355832</td>\n",
       "      <td>2.609716</td>\n",
       "      <td>8.624576</td>\n",
       "      <td>9.371632</td>\n",
       "      <td>11.789219</td>\n",
       "      <td>9.205471</td>\n",
       "      <td>15.204468</td>\n",
       "      <td>8.358906</td>\n",
       "      <td>8.529152</td>\n",
       "      <td>8.021473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.577212</td>\n",
       "      <td>10.795115</td>\n",
       "      <td>24.621388</td>\n",
       "      <td>17.264747</td>\n",
       "      <td>14.221610</td>\n",
       "      <td>8.754692</td>\n",
       "      <td>18.399259</td>\n",
       "      <td>11.358798</td>\n",
       "      <td>15.432650</td>\n",
       "      <td>14.842153</td>\n",
       "      <td>...</td>\n",
       "      <td>15.179359</td>\n",
       "      <td>10.200144</td>\n",
       "      <td>12.645303</td>\n",
       "      <td>12.147416</td>\n",
       "      <td>8.899863</td>\n",
       "      <td>13.954543</td>\n",
       "      <td>12.356942</td>\n",
       "      <td>16.364696</td>\n",
       "      <td>3.817956</td>\n",
       "      <td>4.094035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.299206</td>\n",
       "      <td>13.471215</td>\n",
       "      <td>51.725934</td>\n",
       "      <td>40.786947</td>\n",
       "      <td>26.052414</td>\n",
       "      <td>33.200702</td>\n",
       "      <td>51.014330</td>\n",
       "      <td>27.685009</td>\n",
       "      <td>33.107991</td>\n",
       "      <td>25.359457</td>\n",
       "      <td>...</td>\n",
       "      <td>7.693654</td>\n",
       "      <td>6.359187</td>\n",
       "      <td>5.760296</td>\n",
       "      <td>5.699580</td>\n",
       "      <td>9.895795</td>\n",
       "      <td>17.011648</td>\n",
       "      <td>12.031000</td>\n",
       "      <td>14.637973</td>\n",
       "      <td>10.172737</td>\n",
       "      <td>10.525373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.193955</td>\n",
       "      <td>20.037969</td>\n",
       "      <td>37.780290</td>\n",
       "      <td>28.983748</td>\n",
       "      <td>25.510508</td>\n",
       "      <td>15.970348</td>\n",
       "      <td>27.930757</td>\n",
       "      <td>20.707354</td>\n",
       "      <td>25.341768</td>\n",
       "      <td>27.118987</td>\n",
       "      <td>...</td>\n",
       "      <td>7.068407</td>\n",
       "      <td>7.180632</td>\n",
       "      <td>12.517752</td>\n",
       "      <td>9.026493</td>\n",
       "      <td>7.748172</td>\n",
       "      <td>10.363749</td>\n",
       "      <td>7.099588</td>\n",
       "      <td>12.467672</td>\n",
       "      <td>11.545619</td>\n",
       "      <td>9.098600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0  20.454647  16.739345  39.811892  24.955468  27.088535  17.116793   \n",
       "1  16.175225  10.483281  27.471017  18.509824  19.045353  15.039082   \n",
       "2  10.577212  10.795115  24.621388  17.264747  14.221610   8.754692   \n",
       "3  26.299206  13.471215  51.725934  40.786947  26.052414  33.200702   \n",
       "4  23.193955  20.037969  37.780290  28.983748  25.510508  15.970348   \n",
       "\n",
       "         6          7          8          9    ...        490        491  \\\n",
       "0  25.166957  23.364508  17.223886  18.339175  ...  13.235314   5.160379   \n",
       "1  21.354915  15.790575  13.912508  13.772518  ...  13.355832   2.609716   \n",
       "2  18.399259  11.358798  15.432650  14.842153  ...  15.179359  10.200144   \n",
       "3  51.014330  27.685009  33.107991  25.359457  ...   7.693654   6.359187   \n",
       "4  27.930757  20.707354  25.341768  27.118987  ...   7.068407   7.180632   \n",
       "\n",
       "         492        493        494        495        496        497  \\\n",
       "0  29.194846  17.298314   6.414267   7.780568   6.840910  18.295197   \n",
       "1   8.624576   9.371632  11.789219   9.205471  15.204468   8.358906   \n",
       "2  12.645303  12.147416   8.899863  13.954543  12.356942  16.364696   \n",
       "3   5.760296   5.699580   9.895795  17.011648  12.031000  14.637973   \n",
       "4  12.517752   9.026493   7.748172  10.363749   7.099588  12.467672   \n",
       "\n",
       "         498        499  \n",
       "0  10.014028   6.938318  \n",
       "1   8.529152   8.021473  \n",
       "2   3.817956   4.094035  \n",
       "3  10.172737  10.525373  \n",
       "4  11.545619   9.098600  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0131518f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2557\n",
       "1    2443\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eacf841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Variables Possible:  50\n"
     ]
    }
   ],
   "source": [
    "Gain_Max_Households = 10*1000\n",
    "Max_Variables_Possibles = Gain_Max_Households // 200 \n",
    "print(\"Max Variables Possible: \", Max_Variables_Possibles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f699257",
   "metadata": {},
   "source": [
    "## First algorithm : Logistic regression features + SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def select_features_L1(X_train, y_train, C_feat):\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l1\", solver=\"liblinear\", C=C_feat, random_state=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    coefs = model.coef_.ravel()\n",
    "    selected_indices = np.where(coefs != 0)[0]\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3d51e",
   "metadata": {},
   "source": [
    "### grid-search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e383c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying C_feat = 0.0005, C_svm = 0.01\n",
      "  Fold 1: acc = 0.70, features = 1, gain = 13760.00\n",
      "  Fold 2: acc = 0.74, features = 1, gain = 14600.00\n",
      "  Fold 3: acc = 0.70, features = 1, gain = 13840.00\n",
      "  Fold 4: acc = 0.71, features = 1, gain = 14040.00\n",
      "  Fold 5: acc = 0.71, features = 1, gain = 14000.00\n",
      "  Fold 6: acc = 0.69, features = 1, gain = 13600.00\n",
      "  Fold 7: acc = 0.73, features = 1, gain = 14400.00\n",
      "  Fold 8: acc = 0.69, features = 1, gain = 13680.00\n",
      "  Fold 9: acc = 0.68, features = 1, gain = 13360.00\n",
      "  Fold 10: acc = 0.70, features = 1, gain = 13760.00\n",
      "Average gain for C_feat=0.0005, C_svm=0.01: 13904.00\n",
      "\n",
      "Trying C_feat = 0.0005, C_svm = 0.1\n",
      "  Fold 1: acc = 0.70, features = 1, gain = 13720.00\n",
      "  Fold 2: acc = 0.74, features = 1, gain = 14560.00\n",
      "  Fold 3: acc = 0.70, features = 1, gain = 13760.00\n",
      "  Fold 4: acc = 0.72, features = 1, gain = 14120.00\n",
      "  Fold 5: acc = 0.72, features = 1, gain = 14120.00\n",
      "  Fold 6: acc = 0.69, features = 1, gain = 13680.00\n",
      "  Fold 7: acc = 0.72, features = 1, gain = 14280.00\n",
      "  Fold 8: acc = 0.70, features = 1, gain = 13720.00\n",
      "  Fold 9: acc = 0.68, features = 1, gain = 13480.00\n",
      "  Fold 10: acc = 0.69, features = 1, gain = 13680.00\n",
      "Average gain for C_feat=0.0005, C_svm=0.1: 13912.00\n",
      "\n",
      "Trying C_feat = 0.0005, C_svm = 1.0\n",
      "  Fold 1: acc = 0.70, features = 1, gain = 13720.00\n",
      "  Fold 2: acc = 0.74, features = 1, gain = 14560.00\n",
      "  Fold 3: acc = 0.70, features = 1, gain = 13760.00\n",
      "  Fold 4: acc = 0.72, features = 1, gain = 14160.00\n",
      "  Fold 5: acc = 0.72, features = 1, gain = 14240.00\n",
      "  Fold 6: acc = 0.69, features = 1, gain = 13680.00\n",
      "  Fold 7: acc = 0.72, features = 1, gain = 14280.00\n",
      "  Fold 8: acc = 0.70, features = 1, gain = 13720.00\n",
      "  Fold 9: acc = 0.68, features = 1, gain = 13480.00\n",
      "  Fold 10: acc = 0.69, features = 1, gain = 13680.00\n",
      "Average gain for C_feat=0.0005, C_svm=1.0: 13928.00\n",
      "\n",
      "Trying C_feat = 0.0005, C_svm = 10.0\n",
      "  Fold 1: acc = 0.70, features = 1, gain = 13720.00\n",
      "  Fold 2: acc = 0.74, features = 1, gain = 14560.00\n",
      "  Fold 3: acc = 0.70, features = 1, gain = 13800.00\n",
      "  Fold 4: acc = 0.72, features = 1, gain = 14160.00\n",
      "  Fold 5: acc = 0.72, features = 1, gain = 14240.00\n",
      "  Fold 6: acc = 0.69, features = 1, gain = 13680.00\n",
      "  Fold 7: acc = 0.72, features = 1, gain = 14280.00\n",
      "  Fold 8: acc = 0.70, features = 1, gain = 13720.00\n",
      "  Fold 9: acc = 0.68, features = 1, gain = 13480.00\n",
      "  Fold 10: acc = 0.69, features = 1, gain = 13680.00\n",
      "Average gain for C_feat=0.0005, C_svm=10.0: 13932.00\n",
      "\n",
      "Trying C_feat = 0.0006, C_svm = 0.01\n",
      "  Fold 1: acc = 0.69, features = 4, gain = 13080.00\n",
      "  Fold 2: acc = 0.73, features = 4, gain = 13800.00\n",
      "  Fold 3: acc = 0.70, features = 3, gain = 13480.00\n",
      "  Fold 4: acc = 0.71, features = 2, gain = 13880.00\n",
      "  Fold 5: acc = 0.71, features = 3, gain = 13520.00\n",
      "  Fold 6: acc = 0.69, features = 2, gain = 13480.00\n",
      "  Fold 7: acc = 0.73, features = 2, gain = 14120.00\n",
      "  Fold 8: acc = 0.69, features = 4, gain = 13040.00\n",
      "  Fold 9: acc = 0.68, features = 4, gain = 12880.00\n",
      "  Fold 10: acc = 0.69, features = 4, gain = 13080.00\n",
      "Average gain for C_feat=0.0006, C_svm=0.01: 13436.00\n",
      "\n",
      "Trying C_feat = 0.0006, C_svm = 0.1\n",
      "  Fold 1: acc = 0.70, features = 4, gain = 13120.00\n",
      "  Fold 2: acc = 0.73, features = 4, gain = 13880.00\n",
      "  Fold 3: acc = 0.70, features = 3, gain = 13480.00\n",
      "  Fold 4: acc = 0.72, features = 2, gain = 14000.00\n",
      "  Fold 5: acc = 0.71, features = 3, gain = 13600.00\n",
      "  Fold 6: acc = 0.70, features = 2, gain = 13600.00\n",
      "  Fold 7: acc = 0.72, features = 2, gain = 14000.00\n",
      "  Fold 8: acc = 0.69, features = 4, gain = 13040.00\n",
      "  Fold 9: acc = 0.68, features = 4, gain = 12840.00\n",
      "  Fold 10: acc = 0.69, features = 4, gain = 13080.00\n",
      "Average gain for C_feat=0.0006, C_svm=0.1: 13464.00\n",
      "\n",
      "Trying C_feat = 0.0006, C_svm = 1.0\n",
      "  Fold 1: acc = 0.70, features = 4, gain = 13120.00\n",
      "  Fold 2: acc = 0.73, features = 4, gain = 13880.00\n",
      "  Fold 3: acc = 0.71, features = 3, gain = 13520.00\n",
      "  Fold 4: acc = 0.72, features = 2, gain = 14000.00\n",
      "  Fold 5: acc = 0.71, features = 3, gain = 13600.00\n",
      "  Fold 6: acc = 0.70, features = 2, gain = 13600.00\n",
      "  Fold 7: acc = 0.72, features = 2, gain = 14000.00\n",
      "  Fold 8: acc = 0.69, features = 4, gain = 13000.00\n",
      "  Fold 9: acc = 0.68, features = 4, gain = 12840.00\n",
      "  Fold 10: acc = 0.69, features = 4, gain = 13080.00\n",
      "Average gain for C_feat=0.0006, C_svm=1.0: 13464.00\n",
      "\n",
      "Trying C_feat = 0.0006, C_svm = 10.0\n",
      "  Fold 1: acc = 0.70, features = 4, gain = 13120.00\n",
      "  Fold 2: acc = 0.73, features = 4, gain = 13880.00\n",
      "  Fold 3: acc = 0.71, features = 3, gain = 13520.00\n",
      "  Fold 4: acc = 0.72, features = 2, gain = 14000.00\n",
      "  Fold 5: acc = 0.71, features = 3, gain = 13600.00\n",
      "  Fold 6: acc = 0.70, features = 2, gain = 13600.00\n",
      "  Fold 7: acc = 0.72, features = 2, gain = 14000.00\n",
      "  Fold 8: acc = 0.69, features = 4, gain = 13000.00\n",
      "  Fold 9: acc = 0.68, features = 4, gain = 12840.00\n",
      "  Fold 10: acc = 0.69, features = 4, gain = 13080.00\n",
      "Average gain for C_feat=0.0006, C_svm=10.0: 13464.00\n",
      "\n",
      "Trying C_feat = 0.0007, C_svm = 0.01\n",
      "  Fold 1: acc = 0.70, features = 13, gain = 11320.00\n",
      "  Fold 2: acc = 0.73, features = 12, gain = 12240.00\n",
      "  Fold 3: acc = 0.69, features = 15, gain = 10720.00\n",
      "  Fold 4: acc = 0.71, features = 13, gain = 11680.00\n",
      "  Fold 5: acc = 0.71, features = 11, gain = 11920.00\n",
      "  Fold 6: acc = 0.69, features = 13, gain = 11240.00\n",
      "  Fold 7: acc = 0.72, features = 11, gain = 12160.00\n",
      "  Fold 8: acc = 0.69, features = 12, gain = 11400.00\n",
      "  Fold 9: acc = 0.68, features = 10, gain = 11560.00\n",
      "  Fold 10: acc = 0.69, features = 14, gain = 10920.00\n",
      "Average gain for C_feat=0.0007, C_svm=0.01: 11516.00\n",
      "\n",
      "Trying C_feat = 0.0007, C_svm = 0.1\n",
      "  Fold 1: acc = 0.69, features = 13, gain = 11240.00\n",
      "  Fold 2: acc = 0.73, features = 12, gain = 12240.00\n",
      "  Fold 3: acc = 0.68, features = 15, gain = 10640.00\n",
      "  Fold 4: acc = 0.71, features = 13, gain = 11640.00\n",
      "  Fold 5: acc = 0.71, features = 11, gain = 12040.00\n",
      "  Fold 6: acc = 0.70, features = 13, gain = 11320.00\n",
      "  Fold 7: acc = 0.72, features = 11, gain = 12200.00\n",
      "  Fold 8: acc = 0.69, features = 12, gain = 11360.00\n",
      "  Fold 9: acc = 0.68, features = 10, gain = 11520.00\n",
      "  Fold 10: acc = 0.68, features = 14, gain = 10760.00\n",
      "Average gain for C_feat=0.0007, C_svm=0.1: 11496.00\n",
      "\n",
      "Trying C_feat = 0.0007, C_svm = 1.0\n",
      "  Fold 1: acc = 0.69, features = 13, gain = 11240.00\n",
      "  Fold 2: acc = 0.73, features = 12, gain = 12240.00\n",
      "  Fold 3: acc = 0.68, features = 15, gain = 10640.00\n",
      "  Fold 4: acc = 0.72, features = 13, gain = 11720.00\n",
      "  Fold 5: acc = 0.71, features = 11, gain = 11920.00\n",
      "  Fold 6: acc = 0.70, features = 13, gain = 11320.00\n",
      "  Fold 7: acc = 0.72, features = 11, gain = 12200.00\n",
      "  Fold 8: acc = 0.69, features = 12, gain = 11360.00\n",
      "  Fold 9: acc = 0.68, features = 10, gain = 11560.00\n",
      "  Fold 10: acc = 0.68, features = 14, gain = 10840.00\n",
      "Average gain for C_feat=0.0007, C_svm=1.0: 11504.00\n",
      "\n",
      "Trying C_feat = 0.0007, C_svm = 10.0\n",
      "  Fold 1: acc = 0.69, features = 13, gain = 11240.00\n",
      "  Fold 2: acc = 0.73, features = 12, gain = 12240.00\n",
      "  Fold 3: acc = 0.68, features = 15, gain = 10680.00\n",
      "  Fold 4: acc = 0.72, features = 13, gain = 11720.00\n",
      "  Fold 5: acc = 0.71, features = 11, gain = 11920.00\n",
      "  Fold 6: acc = 0.70, features = 13, gain = 11320.00\n",
      "  Fold 7: acc = 0.72, features = 11, gain = 12200.00\n",
      "  Fold 8: acc = 0.69, features = 12, gain = 11360.00\n",
      "  Fold 9: acc = 0.68, features = 10, gain = 11560.00\n",
      "  Fold 10: acc = 0.68, features = 14, gain = 10840.00\n",
      "Average gain for C_feat=0.0007, C_svm=10.0: 11508.00\n",
      "\n",
      "Trying C_feat = 0.0008, C_svm = 0.01\n",
      "  Fold 1: acc = 0.69, features = 20, gain = 9880.00\n",
      "  Fold 2: acc = 0.73, features = 21, gain = 10360.00\n",
      "  Fold 3: acc = 0.69, features = 20, gain = 9720.00\n",
      "  Fold 4: acc = 0.69, features = 26, gain = 8600.00\n",
      "  Fold 5: acc = 0.70, features = 18, gain = 10320.00\n",
      "  Fold 6: acc = 0.69, features = 20, gain = 9720.00\n",
      "  Fold 7: acc = 0.72, features = 20, gain = 10480.00\n",
      "  Fold 8: acc = 0.69, features = 18, gain = 10160.00\n",
      "  Fold 9: acc = 0.67, features = 19, gain = 9680.00\n",
      "  Fold 10: acc = 0.69, features = 17, gain = 10320.00\n",
      "Average gain for C_feat=0.0008, C_svm=0.01: 9924.00\n",
      "\n",
      "Trying C_feat = 0.0008, C_svm = 0.1\n",
      "  Fold 1: acc = 0.69, features = 20, gain = 9800.00\n",
      "  Fold 2: acc = 0.73, features = 21, gain = 10400.00\n",
      "  Fold 3: acc = 0.69, features = 20, gain = 9800.00\n",
      "  Fold 4: acc = 0.69, features = 26, gain = 8640.00\n",
      "  Fold 5: acc = 0.70, features = 18, gain = 10320.00\n",
      "  Fold 6: acc = 0.69, features = 20, gain = 9840.00\n",
      "  Fold 7: acc = 0.73, features = 20, gain = 10520.00\n",
      "  Fold 8: acc = 0.69, features = 18, gain = 10200.00\n",
      "  Fold 9: acc = 0.67, features = 19, gain = 9640.00\n",
      "  Fold 10: acc = 0.68, features = 17, gain = 10200.00\n",
      "Average gain for C_feat=0.0008, C_svm=0.1: 9936.00\n",
      "\n",
      "Trying C_feat = 0.0008, C_svm = 1.0\n",
      "  Fold 1: acc = 0.69, features = 20, gain = 9760.00\n",
      "  Fold 2: acc = 0.73, features = 21, gain = 10440.00\n",
      "  Fold 3: acc = 0.69, features = 20, gain = 9840.00\n",
      "  Fold 4: acc = 0.69, features = 26, gain = 8640.00\n",
      "  Fold 5: acc = 0.69, features = 18, gain = 10240.00\n",
      "  Fold 6: acc = 0.69, features = 20, gain = 9840.00\n",
      "  Fold 7: acc = 0.73, features = 20, gain = 10520.00\n",
      "  Fold 8: acc = 0.69, features = 18, gain = 10200.00\n",
      "  Fold 9: acc = 0.67, features = 19, gain = 9640.00\n",
      "  Fold 10: acc = 0.68, features = 17, gain = 10240.00\n",
      "Average gain for C_feat=0.0008, C_svm=1.0: 9936.00\n",
      "\n",
      "Trying C_feat = 0.0008, C_svm = 10.0\n",
      "  Fold 1: acc = 0.69, features = 20, gain = 9760.00\n",
      "  Fold 2: acc = 0.73, features = 21, gain = 10440.00\n",
      "  Fold 3: acc = 0.69, features = 20, gain = 9840.00\n",
      "  Fold 4: acc = 0.69, features = 26, gain = 8640.00\n",
      "  Fold 5: acc = 0.69, features = 18, gain = 10240.00\n",
      "  Fold 6: acc = 0.69, features = 20, gain = 9840.00\n",
      "  Fold 7: acc = 0.72, features = 20, gain = 10480.00\n",
      "  Fold 8: acc = 0.69, features = 18, gain = 10200.00\n",
      "  Fold 9: acc = 0.67, features = 19, gain = 9640.00\n",
      "  Fold 10: acc = 0.68, features = 17, gain = 10240.00\n",
      "Average gain for C_feat=0.0008, C_svm=10.0: 9932.00\n",
      "\n",
      "Trying C_feat = 0.0009, C_svm = 0.01\n",
      "  Fold 1: acc = 0.68, features = 26, gain = 8480.00\n",
      "  Fold 2: acc = 0.74, features = 22, gain = 10400.00\n",
      "  Fold 3: acc = 0.69, features = 24, gain = 8960.00\n",
      "  Fold 4: acc = 0.69, features = 29, gain = 8000.00\n",
      "  Fold 5: acc = 0.70, features = 24, gain = 9160.00\n",
      "  Fold 6: acc = 0.70, features = 24, gain = 9120.00\n",
      "  Fold 7: acc = 0.73, features = 26, gain = 9360.00\n",
      "  Fold 8: acc = 0.68, features = 24, gain = 8840.00\n",
      "  Fold 9: acc = 0.67, features = 26, gain = 8200.00\n",
      "  Fold 10: acc = 0.68, features = 24, gain = 8880.00\n",
      "Average gain for C_feat=0.0009, C_svm=0.01: 8940.00\n",
      "\n",
      "Trying C_feat = 0.0009, C_svm = 0.1\n",
      "  Fold 1: acc = 0.69, features = 26, gain = 8640.00\n",
      "  Fold 2: acc = 0.74, features = 22, gain = 10320.00\n",
      "  Fold 3: acc = 0.69, features = 24, gain = 8960.00\n",
      "  Fold 4: acc = 0.70, features = 29, gain = 8160.00\n",
      "  Fold 5: acc = 0.70, features = 24, gain = 9200.00\n",
      "  Fold 6: acc = 0.69, features = 24, gain = 9040.00\n",
      "  Fold 7: acc = 0.73, features = 26, gain = 9440.00\n",
      "  Fold 8: acc = 0.68, features = 24, gain = 8880.00\n",
      "  Fold 9: acc = 0.67, features = 26, gain = 8160.00\n",
      "  Fold 10: acc = 0.68, features = 24, gain = 8880.00\n",
      "Average gain for C_feat=0.0009, C_svm=0.1: 8968.00\n",
      "\n",
      "Trying C_feat = 0.0009, C_svm = 1.0\n",
      "  Fold 1: acc = 0.69, features = 26, gain = 8600.00\n",
      "  Fold 2: acc = 0.74, features = 22, gain = 10360.00\n",
      "  Fold 3: acc = 0.69, features = 24, gain = 9040.00\n",
      "  Fold 4: acc = 0.70, features = 29, gain = 8200.00\n",
      "  Fold 5: acc = 0.70, features = 24, gain = 9240.00\n",
      "  Fold 6: acc = 0.69, features = 24, gain = 9000.00\n",
      "  Fold 7: acc = 0.73, features = 26, gain = 9440.00\n",
      "  Fold 8: acc = 0.69, features = 24, gain = 8960.00\n",
      "  Fold 9: acc = 0.67, features = 26, gain = 8120.00\n",
      "  Fold 10: acc = 0.68, features = 24, gain = 8880.00\n",
      "Average gain for C_feat=0.0009, C_svm=1.0: 8984.00\n",
      "\n",
      "Trying C_feat = 0.0009, C_svm = 10.0\n",
      "  Fold 1: acc = 0.69, features = 26, gain = 8600.00\n",
      "  Fold 2: acc = 0.74, features = 22, gain = 10360.00\n",
      "  Fold 3: acc = 0.69, features = 24, gain = 9040.00\n",
      "  Fold 4: acc = 0.70, features = 29, gain = 8200.00\n",
      "  Fold 5: acc = 0.70, features = 24, gain = 9200.00\n",
      "  Fold 6: acc = 0.69, features = 24, gain = 9040.00\n",
      "  Fold 7: acc = 0.73, features = 26, gain = 9440.00\n",
      "  Fold 8: acc = 0.69, features = 24, gain = 9000.00\n",
      "  Fold 9: acc = 0.67, features = 26, gain = 8120.00\n",
      "  Fold 10: acc = 0.69, features = 24, gain = 8920.00\n",
      "Average gain for C_feat=0.0009, C_svm=10.0: 8992.00\n",
      "\n",
      "Trying C_feat = 0.001, C_svm = 0.01\n",
      "  Fold 1: acc = 0.69, features = 29, gain = 7920.00\n",
      "  Fold 2: acc = 0.71, features = 29, gain = 8480.00\n",
      "  Fold 3: acc = 0.69, features = 28, gain = 8160.00\n",
      "  Fold 4: acc = 0.69, features = 29, gain = 8000.00\n",
      "  Fold 5: acc = 0.70, features = 31, gain = 7880.00\n",
      "  Fold 6: acc = 0.68, features = 30, gain = 7680.00\n",
      "  Fold 7: acc = 0.73, features = 30, gain = 8600.00\n",
      "  Fold 8: acc = 0.68, features = 31, gain = 7480.00\n",
      "  Fold 9: acc = 0.67, features = 31, gain = 7160.00\n",
      "  Fold 10: acc = 0.68, features = 25, gain = 8680.00\n",
      "Average gain for C_feat=0.001, C_svm=0.01: 8004.00\n",
      "\n",
      "Trying C_feat = 0.001, C_svm = 0.1\n",
      "  Fold 1: acc = 0.69, features = 29, gain = 7920.00\n",
      "  Fold 2: acc = 0.72, features = 29, gain = 8680.00\n",
      "  Fold 3: acc = 0.69, features = 28, gain = 8200.00\n",
      "  Fold 4: acc = 0.70, features = 29, gain = 8160.00\n",
      "  Fold 5: acc = 0.70, features = 31, gain = 7880.00\n",
      "  Fold 6: acc = 0.69, features = 30, gain = 7720.00\n",
      "  Fold 7: acc = 0.73, features = 30, gain = 8600.00\n",
      "  Fold 8: acc = 0.69, features = 31, gain = 7600.00\n",
      "  Fold 9: acc = 0.66, features = 31, gain = 7080.00\n",
      "  Fold 10: acc = 0.69, features = 25, gain = 8760.00\n",
      "Average gain for C_feat=0.001, C_svm=0.1: 8060.00\n",
      "\n",
      "Trying C_feat = 0.001, C_svm = 1.0\n",
      "  Fold 1: acc = 0.69, features = 29, gain = 7960.00\n",
      "  Fold 2: acc = 0.73, features = 29, gain = 8760.00\n",
      "  Fold 3: acc = 0.69, features = 28, gain = 8200.00\n",
      "  Fold 4: acc = 0.70, features = 29, gain = 8200.00\n",
      "  Fold 5: acc = 0.70, features = 31, gain = 7840.00\n",
      "  Fold 6: acc = 0.69, features = 30, gain = 7720.00\n",
      "  Fold 7: acc = 0.72, features = 30, gain = 8480.00\n",
      "  Fold 8: acc = 0.69, features = 31, gain = 7600.00\n",
      "  Fold 9: acc = 0.66, features = 31, gain = 7080.00\n",
      "  Fold 10: acc = 0.69, features = 25, gain = 8720.00\n",
      "Average gain for C_feat=0.001, C_svm=1.0: 8056.00\n",
      "\n",
      "Trying C_feat = 0.001, C_svm = 10.0\n",
      "  Fold 1: acc = 0.69, features = 29, gain = 7960.00\n",
      "  Fold 2: acc = 0.73, features = 29, gain = 8760.00\n",
      "  Fold 3: acc = 0.69, features = 28, gain = 8200.00\n",
      "  Fold 4: acc = 0.70, features = 29, gain = 8200.00\n",
      "  Fold 5: acc = 0.70, features = 31, gain = 7880.00\n",
      "  Fold 6: acc = 0.69, features = 30, gain = 7720.00\n",
      "  Fold 7: acc = 0.73, features = 30, gain = 8520.00\n",
      "  Fold 8: acc = 0.69, features = 31, gain = 7600.00\n",
      "  Fold 9: acc = 0.66, features = 31, gain = 7080.00\n",
      "  Fold 10: acc = 0.68, features = 25, gain = 8680.00\n",
      "Average gain for C_feat=0.001, C_svm=10.0: 8060.00\n",
      "\n",
      "Best configuration: C_feat=0.0005, C_svm=10.0 with gain: 13932.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = df_variables.values\n",
    "y = df_labels.values.ravel()\n",
    "\n",
    "C_feat_grid = [0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001]\n",
    "C_svm_grid = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "best_config = None\n",
    "best_gain = -np.inf\n",
    "\n",
    "for C_feat in C_feat_grid:\n",
    "    for C_svm in C_svm_grid:\n",
    "        print(f\"\\nTrying C_feat = {C_feat}, C_svm = {C_svm}\")\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        Average_Predicted_Gain = 0\n",
    "\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            selected_features = select_features_L1(X_train, y_train, C_feat)\n",
    "            num_selected = len(selected_features)\n",
    "\n",
    "            if num_selected == 0:\n",
    "                print(f\"  Fold {index}: no features selected, skipping\")\n",
    "                continue\n",
    "\n",
    "            model = make_pipeline(\n",
    "                StandardScaler(),\n",
    "                svm.SVC(kernel=\"linear\", C=C_svm, probability=True, random_state=42),\n",
    "            )\n",
    "            model.fit(X_train[:, selected_features], y_train)\n",
    "            y_pred = model.predict(X_test[:, selected_features])\n",
    "\n",
    "            predicted_cash = accuracy_score(y_test, y_pred) * 10 * 1000\n",
    "            Cost = 200 * num_selected\n",
    "            predicted_gain = predicted_cash - Cost\n",
    "            print(\n",
    "                f\"  Fold {index}: acc = {accuracy_score(y_test, y_pred):.2f}, \"\n",
    "                f\"features = {num_selected}, gain = {predicted_gain:.2f}\"\n",
    "            )\n",
    "\n",
    "            Average_Predicted_Gain += predicted_gain\n",
    "\n",
    "        Average_Predicted_Gain /= skf.get_n_splits()\n",
    "        print(\n",
    "            f\"Average gain for C_feat={C_feat}, C_svm={C_svm}: {Average_Predicted_Gain:.2f}\"\n",
    "        )\n",
    "\n",
    "        if Average_Predicted_Gain > best_gain:\n",
    "            best_gain = Average_Predicted_Gain\n",
    "            best_config = (C_feat, C_svm)\n",
    "\n",
    "print(\n",
    "    f\"\\nBest configuration: C_feat={best_config[0]}, C_svm={best_config[1]} with gain: {best_gain:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932a89e",
   "metadata": {},
   "source": [
    "## Second algorithm : Mutual Information + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ab419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_features_MI(X_train, y_train, threshold):\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_train, y_train, discrete_features=False, random_state=0\n",
    "    )\n",
    "\n",
    "    selected_indices = [i for i, score in enumerate(mi_scores) if score > threshold]\n",
    "\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a6369",
   "metadata": {},
   "source": [
    "### grid search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying C_feat = 0.06, C_svm = 0.01\n",
      "  Fold 1: acc = 0.68, features = 4, gain = 12760.00\n",
      "  Fold 2: acc = 0.75, features = 3, gain = 14360.00\n",
      "  Fold 3: acc = 0.70, features = 6, gain = 12880.00\n",
      "  Fold 4: acc = 0.70, features = 5, gain = 13000.00\n",
      "  Fold 5: acc = 0.71, features = 4, gain = 13400.00\n",
      "  Fold 6: acc = 0.68, features = 4, gain = 12800.00\n",
      "  Fold 7: acc = 0.72, features = 5, gain = 13480.00\n",
      "  Fold 8: acc = 0.69, features = 5, gain = 12760.00\n",
      "  Fold 9: acc = 0.67, features = 5, gain = 12480.00\n",
      "  Fold 10: acc = 0.70, features = 3, gain = 13320.00\n",
      "Average gain for C_feat=0.06, C_svm=0.01: 13124.00\n",
      "\n",
      "Trying C_feat = 0.06, C_svm = 0.1\n",
      "  Fold 1: acc = 0.71, features = 4, gain = 13320.00\n",
      "  Fold 2: acc = 0.74, features = 3, gain = 14240.00\n",
      "  Fold 3: acc = 0.70, features = 6, gain = 12760.00\n",
      "  Fold 4: acc = 0.72, features = 5, gain = 13320.00\n",
      "  Fold 5: acc = 0.73, features = 4, gain = 13760.00\n",
      "  Fold 6: acc = 0.71, features = 4, gain = 13400.00\n",
      "  Fold 7: acc = 0.72, features = 5, gain = 13440.00\n",
      "  Fold 8: acc = 0.71, features = 5, gain = 13200.00\n",
      "  Fold 9: acc = 0.69, features = 5, gain = 12840.00\n",
      "  Fold 10: acc = 0.70, features = 3, gain = 13400.00\n",
      "Average gain for C_feat=0.06, C_svm=0.1: 13368.00\n",
      "\n",
      "Trying C_feat = 0.06, C_svm = 1.0\n",
      "  Fold 1: acc = 0.71, features = 4, gain = 13360.00\n",
      "  Fold 2: acc = 0.73, features = 3, gain = 14040.00\n",
      "  Fold 3: acc = 0.69, features = 6, gain = 12600.00\n",
      "  Fold 4: acc = 0.72, features = 5, gain = 13400.00\n",
      "  Fold 5: acc = 0.74, features = 4, gain = 13920.00\n",
      "  Fold 6: acc = 0.72, features = 4, gain = 13600.00\n",
      "  Fold 7: acc = 0.73, features = 5, gain = 13600.00\n",
      "  Fold 8: acc = 0.71, features = 5, gain = 13280.00\n",
      "  Fold 9: acc = 0.69, features = 5, gain = 12800.00\n",
      "  Fold 10: acc = 0.71, features = 3, gain = 13520.00\n",
      "Average gain for C_feat=0.06, C_svm=1.0: 13412.00\n",
      "\n",
      "Trying C_feat = 0.06, C_svm = 10.0\n",
      "  Fold 1: acc = 0.71, features = 4, gain = 13320.00\n",
      "  Fold 2: acc = 0.73, features = 3, gain = 14040.00\n",
      "  Fold 3: acc = 0.69, features = 6, gain = 12600.00\n",
      "  Fold 4: acc = 0.72, features = 5, gain = 13400.00\n",
      "  Fold 5: acc = 0.74, features = 4, gain = 13920.00\n",
      "  Fold 6: acc = 0.72, features = 4, gain = 13600.00\n",
      "  Fold 7: acc = 0.73, features = 5, gain = 13640.00\n",
      "  Fold 8: acc = 0.71, features = 5, gain = 13280.00\n",
      "  Fold 9: acc = 0.69, features = 5, gain = 12840.00\n",
      "  Fold 10: acc = 0.71, features = 3, gain = 13520.00\n",
      "Average gain for C_feat=0.06, C_svm=10.0: 13416.00\n",
      "\n",
      "Trying C_feat = 0.05, C_svm = 0.01\n",
      "  Fold 1: acc = 0.69, features = 7, gain = 12440.00\n",
      "  Fold 2: acc = 0.74, features = 5, gain = 13720.00\n",
      "  Fold 3: acc = 0.71, features = 7, gain = 12720.00\n",
      "  Fold 4: acc = 0.70, features = 8, gain = 12320.00\n",
      "  Fold 5: acc = 0.71, features = 6, gain = 13040.00\n",
      "  Fold 6: acc = 0.69, features = 7, gain = 12360.00\n",
      "  Fold 7: acc = 0.72, features = 7, gain = 12960.00\n",
      "  Fold 8: acc = 0.70, features = 9, gain = 12160.00\n",
      "  Fold 9: acc = 0.68, features = 8, gain = 12040.00\n",
      "  Fold 10: acc = 0.70, features = 6, gain = 12720.00\n",
      "Average gain for C_feat=0.05, C_svm=0.01: 12648.00\n",
      "\n",
      "Trying C_feat = 0.05, C_svm = 0.1\n",
      "  Fold 1: acc = 0.71, features = 7, gain = 12800.00\n",
      "  Fold 2: acc = 0.74, features = 5, gain = 13880.00\n",
      "  Fold 3: acc = 0.70, features = 7, gain = 12560.00\n",
      "  Fold 4: acc = 0.72, features = 8, gain = 12760.00\n",
      "  Fold 5: acc = 0.73, features = 6, gain = 13320.00\n",
      "  Fold 6: acc = 0.71, features = 7, gain = 12880.00\n",
      "  Fold 7: acc = 0.73, features = 7, gain = 13200.00\n",
      "  Fold 8: acc = 0.71, features = 9, gain = 12440.00\n",
      "  Fold 9: acc = 0.69, features = 8, gain = 12240.00\n",
      "  Fold 10: acc = 0.70, features = 6, gain = 12880.00\n",
      "Average gain for C_feat=0.05, C_svm=0.1: 12896.00\n",
      "\n",
      "Trying C_feat = 0.05, C_svm = 1.0\n",
      "  Fold 1: acc = 0.71, features = 7, gain = 12720.00\n",
      "  Fold 2: acc = 0.74, features = 5, gain = 13880.00\n",
      "  Fold 3: acc = 0.69, features = 7, gain = 12360.00\n",
      "  Fold 4: acc = 0.73, features = 8, gain = 12920.00\n",
      "  Fold 5: acc = 0.73, features = 6, gain = 13440.00\n",
      "  Fold 6: acc = 0.72, features = 7, gain = 12920.00\n",
      "  Fold 7: acc = 0.73, features = 7, gain = 13120.00\n",
      "  Fold 8: acc = 0.72, features = 9, gain = 12520.00\n",
      "  Fold 9: acc = 0.70, features = 8, gain = 12320.00\n",
      "  Fold 10: acc = 0.71, features = 6, gain = 12920.00\n",
      "Average gain for C_feat=0.05, C_svm=1.0: 12912.00\n",
      "\n",
      "Trying C_feat = 0.05, C_svm = 10.0\n",
      "  Fold 1: acc = 0.70, features = 7, gain = 12640.00\n",
      "  Fold 2: acc = 0.74, features = 5, gain = 13880.00\n",
      "  Fold 3: acc = 0.69, features = 7, gain = 12320.00\n",
      "  Fold 4: acc = 0.73, features = 8, gain = 12920.00\n",
      "  Fold 5: acc = 0.73, features = 6, gain = 13440.00\n",
      "  Fold 6: acc = 0.72, features = 7, gain = 12920.00\n",
      "  Fold 7: acc = 0.73, features = 7, gain = 13120.00\n",
      "  Fold 8: acc = 0.72, features = 9, gain = 12560.00\n",
      "  Fold 9: acc = 0.70, features = 8, gain = 12440.00\n",
      "  Fold 10: acc = 0.70, features = 6, gain = 12880.00\n",
      "Average gain for C_feat=0.05, C_svm=10.0: 12912.00\n",
      "\n",
      "Trying C_feat = 0.04, C_svm = 0.01\n",
      "  Fold 1: acc = 0.69, features = 10, gain = 11880.00\n",
      "  Fold 2: acc = 0.74, features = 10, gain = 12880.00\n",
      "  Fold 3: acc = 0.71, features = 9, gain = 12360.00\n",
      "  Fold 4: acc = 0.70, features = 10, gain = 11960.00\n",
      "  Fold 5: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 6: acc = 0.69, features = 10, gain = 11760.00\n",
      "  Fold 7: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 8: acc = 0.69, features = 10, gain = 11840.00\n",
      "  Fold 9: acc = 0.68, features = 10, gain = 11640.00\n",
      "  Fold 10: acc = 0.69, features = 10, gain = 11880.00\n",
      "Average gain for C_feat=0.04, C_svm=0.01: 12068.00\n",
      "\n",
      "Trying C_feat = 0.04, C_svm = 0.1\n",
      "  Fold 1: acc = 0.71, features = 10, gain = 12200.00\n",
      "  Fold 2: acc = 0.75, features = 10, gain = 12920.00\n",
      "  Fold 3: acc = 0.70, features = 9, gain = 12240.00\n",
      "  Fold 4: acc = 0.72, features = 10, gain = 12320.00\n",
      "  Fold 5: acc = 0.73, features = 10, gain = 12600.00\n",
      "  Fold 6: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 7: acc = 0.73, features = 10, gain = 12640.00\n",
      "  Fold 8: acc = 0.72, features = 10, gain = 12320.00\n",
      "  Fold 9: acc = 0.70, features = 10, gain = 11920.00\n",
      "  Fold 10: acc = 0.70, features = 10, gain = 12080.00\n",
      "Average gain for C_feat=0.04, C_svm=0.1: 12348.00\n",
      "\n",
      "Trying C_feat = 0.04, C_svm = 1.0\n",
      "  Fold 1: acc = 0.71, features = 10, gain = 12280.00\n",
      "  Fold 2: acc = 0.74, features = 10, gain = 12800.00\n",
      "  Fold 3: acc = 0.70, features = 9, gain = 12200.00\n",
      "  Fold 4: acc = 0.71, features = 10, gain = 12280.00\n",
      "  Fold 5: acc = 0.73, features = 10, gain = 12520.00\n",
      "  Fold 6: acc = 0.72, features = 10, gain = 12360.00\n",
      "  Fold 7: acc = 0.74, features = 10, gain = 12720.00\n",
      "  Fold 8: acc = 0.71, features = 10, gain = 12280.00\n",
      "  Fold 9: acc = 0.70, features = 10, gain = 12080.00\n",
      "  Fold 10: acc = 0.71, features = 10, gain = 12120.00\n",
      "Average gain for C_feat=0.04, C_svm=1.0: 12364.00\n",
      "\n",
      "Trying C_feat = 0.04, C_svm = 10.0\n",
      "  Fold 1: acc = 0.72, features = 10, gain = 12320.00\n",
      "  Fold 2: acc = 0.73, features = 10, gain = 12680.00\n",
      "  Fold 3: acc = 0.70, features = 9, gain = 12160.00\n",
      "  Fold 4: acc = 0.72, features = 10, gain = 12320.00\n",
      "  Fold 5: acc = 0.73, features = 10, gain = 12520.00\n",
      "  Fold 6: acc = 0.72, features = 10, gain = 12320.00\n",
      "  Fold 7: acc = 0.73, features = 10, gain = 12600.00\n",
      "  Fold 8: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 9: acc = 0.71, features = 10, gain = 12160.00\n",
      "  Fold 10: acc = 0.71, features = 10, gain = 12120.00\n",
      "Average gain for C_feat=0.04, C_svm=10.0: 12344.00\n",
      "\n",
      "Trying C_feat = 0.03, C_svm = 0.01\n",
      "  Fold 1: acc = 0.70, features = 12, gain = 11560.00\n",
      "  Fold 2: acc = 0.74, features = 10, gain = 12880.00\n",
      "  Fold 3: acc = 0.70, features = 10, gain = 11960.00\n",
      "  Fold 4: acc = 0.70, features = 11, gain = 11880.00\n",
      "  Fold 5: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 6: acc = 0.69, features = 10, gain = 11760.00\n",
      "  Fold 7: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 8: acc = 0.70, features = 11, gain = 11760.00\n",
      "  Fold 9: acc = 0.68, features = 11, gain = 11400.00\n",
      "  Fold 10: acc = 0.69, features = 10, gain = 11880.00\n",
      "Average gain for C_feat=0.03, C_svm=0.01: 11956.00\n",
      "\n",
      "Trying C_feat = 0.03, C_svm = 0.1\n",
      "  Fold 1: acc = 0.71, features = 12, gain = 11880.00\n",
      "  Fold 2: acc = 0.75, features = 10, gain = 12920.00\n",
      "  Fold 3: acc = 0.70, features = 10, gain = 12040.00\n",
      "  Fold 4: acc = 0.72, features = 11, gain = 12280.00\n",
      "  Fold 5: acc = 0.73, features = 10, gain = 12600.00\n",
      "  Fold 6: acc = 0.71, features = 10, gain = 12240.00\n",
      "  Fold 7: acc = 0.73, features = 10, gain = 12640.00\n",
      "  Fold 8: acc = 0.72, features = 11, gain = 12120.00\n",
      "  Fold 9: acc = 0.69, features = 11, gain = 11680.00\n",
      "  Fold 10: acc = 0.70, features = 10, gain = 12080.00\n",
      "Average gain for C_feat=0.03, C_svm=0.1: 12248.00\n",
      "\n",
      "Trying C_feat = 0.03, C_svm = 1.0\n",
      "  Fold 1: acc = 0.72, features = 12, gain = 11960.00\n",
      "  Fold 2: acc = 0.74, features = 10, gain = 12800.00\n",
      "  Fold 3: acc = 0.70, features = 10, gain = 11960.00\n",
      "  Fold 4: acc = 0.72, features = 11, gain = 12120.00\n",
      "  Fold 5: acc = 0.73, features = 10, gain = 12520.00\n",
      "  Fold 6: acc = 0.72, features = 10, gain = 12360.00\n",
      "  Fold 7: acc = 0.74, features = 10, gain = 12720.00\n",
      "  Fold 8: acc = 0.71, features = 11, gain = 12080.00\n",
      "  Fold 9: acc = 0.71, features = 11, gain = 11920.00\n",
      "  Fold 10: acc = 0.71, features = 10, gain = 12120.00\n",
      "Average gain for C_feat=0.03, C_svm=1.0: 12256.00\n",
      "\n",
      "Trying C_feat = 0.03, C_svm = 10.0\n",
      "  Fold 1: acc = 0.72, features = 12, gain = 11920.00\n",
      "  Fold 2: acc = 0.73, features = 10, gain = 12680.00\n",
      "  Fold 3: acc = 0.70, features = 10, gain = 11960.00\n",
      "  Fold 4: acc = 0.71, features = 11, gain = 12080.00\n",
      "  Fold 5: acc = 0.73, features = 10, gain = 12520.00\n",
      "  Fold 6: acc = 0.72, features = 10, gain = 12320.00\n",
      "  Fold 7: acc = 0.73, features = 10, gain = 12600.00\n",
      "  Fold 8: acc = 0.71, features = 11, gain = 12040.00\n",
      "  Fold 9: acc = 0.71, features = 11, gain = 11960.00\n",
      "  Fold 10: acc = 0.71, features = 10, gain = 12120.00\n",
      "Average gain for C_feat=0.03, C_svm=10.0: 12220.00\n",
      "\n",
      "Best configuration: C_feat=0.06, C_svm=10.0 with gain: 13416.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = df_variables.values\n",
    "y = df_labels.values.ravel()\n",
    "\n",
    "C_feat_grid = [0.06,0.05,0.04,0.03]\n",
    "\n",
    "C_svm_grid = [ 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "best_config = None\n",
    "best_gain = -np.inf\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_features_MI(X_train, y_train, threshold):\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_train, y_train, discrete_features=False, random_state=0\n",
    "    )\n",
    "\n",
    "    selected_indices = [i for i, score in enumerate(mi_scores) if score > threshold]\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "for C_feat in C_feat_grid:\n",
    "    for C_svm in C_svm_grid:\n",
    "        print(f\"\\nTrying C_feat = {C_feat}, C_svm = {C_svm}\")\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        Average_Predicted_Gain = 0\n",
    "\n",
    "        for index, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            \n",
    "            selected_features = select_features_MI(X_train, y_train, C_feat)\n",
    "            \n",
    "            num_selected = len(selected_features)\n",
    "\n",
    "            if num_selected == 0:\n",
    "                print(f\"  Fold {index}: no features selected, skipping\")\n",
    "                continue\n",
    "\n",
    "            model = make_pipeline(\n",
    "                StandardScaler(),\n",
    "                svm.SVC(kernel=\"linear\", C=C_svm, probability=True, random_state=42),\n",
    "            )\n",
    "            model.fit(X_train[:, selected_features], y_train)\n",
    "            y_pred = model.predict(X_test[:, selected_features])\n",
    "\n",
    "            predicted_cash = accuracy_score(y_test, y_pred) * 10 * 1000\n",
    "            Cost = 200 * num_selected\n",
    "            predicted_gain = predicted_cash - Cost\n",
    "            print(\n",
    "                f\"  Fold {index}: acc = {accuracy_score(y_test, y_pred):.2f}, \"\n",
    "                f\"features = {num_selected}, gain = {predicted_gain:.2f}\"\n",
    "            )\n",
    "\n",
    "            Average_Predicted_Gain += predicted_gain\n",
    "\n",
    "        Average_Predicted_Gain /= skf.get_n_splits()\n",
    "        print(\n",
    "            f\"Average gain for C_feat={C_feat}, C_svm={C_svm}: {Average_Predicted_Gain:.2f}\"\n",
    "        )\n",
    "\n",
    "        if Average_Predicted_Gain > best_gain:\n",
    "            best_gain = Average_Predicted_Gain\n",
    "            best_config = (C_feat, C_svm)\n",
    "\n",
    "print(\n",
    "    f\"\\nBest configuration: C_feat={best_config[0]}, C_svm={best_config[1]} with gain: {best_gain:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43093229",
   "metadata": {},
   "source": [
    "## Third algorithm : Mutual Information + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "401e188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_features_MI(X_train, y_train, threshold=0.01):\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_train, y_train, discrete_features=False, random_state=0\n",
    "    )\n",
    "\n",
    "    selected_indices = [i for i, score in enumerate(mi_scores) if score > threshold]\n",
    "\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c53924",
   "metadata": {},
   "source": [
    "### Grid-search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba29e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e8d892",
   "metadata": {},
   "source": [
    "## Fourth algorithm : Mutual Information + XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85b3ef",
   "metadata": {},
   "source": [
    "## Fifth algorithm : Random Forest Feature Importance + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394a161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebb0a03",
   "metadata": {},
   "source": [
    "### Leaderboard part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3af8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1000 clients selected.\n"
     ]
    }
   ],
   "source": [
    "selected_features_final = select_features_MI(X, y, threshold=0.01)\n",
    "model.fit(X[:, selected_features_final], y)\n",
    "\n",
    "X_clients = df_variables_clients.values\n",
    "proba_preds = model.predict_proba(X_clients[:, selected_features_final])[:, 1]\n",
    "\n",
    "\n",
    "top_1000_indices = np.argsort(proba_preds)[-1000:][::-1]\n",
    "top_1000_clients = df_variables_clients.iloc[top_1000_indices]\n",
    "\n",
    "print(\"Top 1000 clients selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28305718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 24,\n",
       " 28,\n",
       " 34,\n",
       " 39,\n",
       " 45,\n",
       " 48,\n",
       " 75,\n",
       " 90,\n",
       " 94,\n",
       " 100,\n",
       " 121,\n",
       " 135,\n",
       " 136,\n",
       " 171,\n",
       " 226,\n",
       " 238,\n",
       " 242,\n",
       " 245,\n",
       " 263,\n",
       " 287,\n",
       " 313,\n",
       " 314,\n",
       " 318,\n",
       " 323,\n",
       " 356,\n",
       " 376,\n",
       " 386,\n",
       " 387,\n",
       " 405,\n",
       " 409,\n",
       " 413,\n",
       " 414,\n",
       " 416,\n",
       " 425,\n",
       " 435,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 448,\n",
       " 460,\n",
       " 462,\n",
       " 471,\n",
       " 484,\n",
       " 493,\n",
       " 495]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05fe191b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4755, 3866, 2792, 2084, 3780, 4354,  281, 3347, 2735, 4430, 1361,\n",
       "       3789, 3447, 1020, 2822, 4674, 4068, 3509,  405, 2268, 1288, 4880,\n",
       "       4008, 2656,   79, 3147, 4075, 3654, 4344, 2983, 4093, 3383, 3571,\n",
       "       1622,  512, 2576, 3070, 4550, 1734, 3250, 3672,    2, 3375, 2803,\n",
       "       4192,  384, 3449, 1868, 1866, 2216, 2689, 3954,  883, 1830, 3020,\n",
       "       1057, 4829, 2908,  681, 3115, 3774, 3942, 4480, 1053, 2481, 4479,\n",
       "       4123,  254, 3312, 4551, 1805, 2368, 4235, 2707, 3825, 2386, 4445,\n",
       "       4290, 3451,  129, 4745, 2456, 4895,   96, 3784,  415, 2020, 1286,\n",
       "       1711, 2243,  492, 3521,  280, 2612, 3693, 2424, 2997, 2031, 1474,\n",
       "       1493, 3114, 4429, 4908,   80, 4856, 1094, 1756, 1165, 1606, 1170,\n",
       "       1872, 3182,  109, 2985, 1826, 2321, 3444, 4470, 3483, 2700, 4109,\n",
       "       3439, 1320,  207, 1137, 3833,  149, 4749, 4519, 3335, 1243,  868,\n",
       "        549, 2431, 3283, 4009, 4619, 1428, 2292, 4754, 1233, 2353, 3925,\n",
       "       4320, 2489, 3120,  524,  920, 1246, 1559, 1777, 3063, 2271, 3198,\n",
       "        749, 4273, 1498, 4859, 4683, 4598, 4027, 3178, 3576,  987, 2824,\n",
       "       3939, 2669, 4766, 1446, 4671, 4617, 1408, 2029, 2342,  232, 2712,\n",
       "       2654,  791, 2328, 3510, 1938, 3535, 2298, 2334, 2993, 4271, 3356,\n",
       "       4452,  490, 1127, 1698, 1825, 3333, 3000, 2620, 2288, 2496, 2514,\n",
       "       2203, 1534, 4304, 4141, 1003, 1561, 1573, 1335, 4635,  451, 1216,\n",
       "       3112,  550, 4331, 1525,  662, 2634, 1476,  395, 3762, 2402, 4952,\n",
       "       4780, 4915, 2859, 3832,  474, 2396, 2211, 3583, 3841, 4018, 3799,\n",
       "       1356, 1369, 3445, 3166, 4831, 2398, 1457,  273, 3806, 1731, 1585,\n",
       "        163, 1256, 1407, 3321, 3485, 4775, 3441,  131, 2916, 2749, 3906,\n",
       "       3080,   98, 1254, 2195, 1907, 1888, 4826, 2374, 1890,  967, 2995,\n",
       "       3872, 1577, 2267, 1689, 3793, 4922, 1782, 4538, 2726,  107, 2556,\n",
       "       4345, 2827, 4050, 3965, 2617, 4601, 4882, 2302, 3481, 2949, 1469,\n",
       "       1723, 2042, 4910,  270, 3272, 2218, 1672, 2635,  278,  664,   36,\n",
       "       1774, 2337,  836,  707, 3327, 3458, 4394, 4802,  693, 4920, 1199,\n",
       "       4723, 4586, 3743, 4762, 1375, 1732, 4120, 2454, 1079, 2755, 1722,\n",
       "        108,  555, 1611, 4314, 1709, 4262, 4337, 4197, 4956, 3977,  253,\n",
       "       3573,  698,  685, 2798, 3243, 3038, 1391,   23, 3116, 2221, 1755,\n",
       "       3554, 4297, 3052,  882, 4168, 4605, 1550, 2886, 4873, 4306, 4720,\n",
       "       3099,  824, 2004, 1969, 3392, 2129, 4102, 3282,  313, 3015, 3608,\n",
       "       2618,  667,  432,   62, 3524, 1092, 1554, 3537, 4760,  961, 1983,\n",
       "       3201, 1300, 2294, 2363, 3958, 4566, 4139, 2043, 4953, 4411,  319,\n",
       "       1309, 1135, 4627, 2570, 1879,  369, 1331, 4143, 3933, 3126, 1547,\n",
       "       1638, 4535, 1100, 4184, 1043, 1651, 2699, 2233, 3151, 4549, 3432,\n",
       "       4425, 3859, 4088, 4666,  618, 2452, 2001, 2086, 1030, 4443, 2361,\n",
       "       2095, 1842, 2155, 1966,  843,  683, 2313, 2499, 2279, 4997, 1700,\n",
       "       1790, 4737, 1984,  726, 1238, 3141, 3009, 3912, 1745, 2116, 2470,\n",
       "       4420,  431, 3315, 1419, 1306, 1315, 2554,  140, 4350,  289, 1218,\n",
       "       1742, 3989,  493, 2984,  875, 4446, 2577, 3372, 1152,  630, 1632,\n",
       "       1555,  279, 4892, 1149, 1529, 3489, 2978, 2928, 3790, 2825, 2567,\n",
       "        584, 4969, 3998, 4475, 1490, 1077, 3526, 3360, 1289, 3412,  421,\n",
       "        467, 1180, 1539, 1345, 3468,  566, 2641,  372, 1177, 3505, 2604,\n",
       "       1903, 1892, 1344, 2360, 1610, 3331,  861, 1724,  894, 1761, 4663,\n",
       "       2161,   84, 2642,  489, 3626, 2864,  601, 2349, 2657, 3953, 1786,\n",
       "       3858, 4837, 1816, 2222,  854, 3270, 3777, 4386,  433, 2912,  527,\n",
       "        391, 1368, 3219, 1093, 2014,  103,  748, 2723, 3727, 1283, 4371,\n",
       "       3092, 3423, 2287, 1338, 1049, 3675, 1667, 1461, 1435, 2372, 1008,\n",
       "       3898, 2509, 3330, 4542, 3985, 2273, 1187, 3294,  985, 4821, 4693,\n",
       "       4338, 2283, 2331,   68, 2667, 2857, 2541,  132, 4277, 3303, 3403,\n",
       "       2765, 1067, 4869, 4991, 1874, 4552, 1543, 3440,  696, 4797, 3139,\n",
       "       3579, 2410, 2519,  331, 1497,  574, 4439,  202, 4854, 4159, 3026,\n",
       "       3893, 4148, 1992, 3856, 2285, 4492, 2791, 4264, 2008,  531, 2788,\n",
       "        953, 3870, 1873, 3874,  330, 3492, 4381, 3689, 1025, 1102, 2165,\n",
       "       4876,  521, 3068, 4388, 1068, 1116, 3941,  604, 3857, 4214, 1047,\n",
       "        671, 3704, 2622, 3001, 2235, 3353, 4078, 1925,  485, 2091, 2383,\n",
       "       2026, 3722, 4056,  208, 2394,  673,  394, 3538, 4798, 3061, 2047,\n",
       "       4210, 1614, 4514, 3497,  558, 4534, 3479, 3467, 4539, 4369,  901,\n",
       "       4353,  271, 3277, 4993, 2186, 3456, 4253,   94, 1949, 1591, 4163,\n",
       "       3252, 1491,  347, 4899, 1410, 3827,  647, 4126, 4365, 3078,  734,\n",
       "       1930, 4958, 3017, 2706, 4387,  426, 1947,  101, 2469,  420, 3402,\n",
       "       2961, 4072, 2087, 1640,  790, 3205,  758, 1883, 2423,   60, 4688,\n",
       "        687, 1386, 2125, 4039, 4809, 2821, 2662, 1743, 3048, 2063, 2484,\n",
       "       2747,  366, 2796, 4372, 1239, 3588, 2780, 3003, 3083, 2903, 4501,\n",
       "       4545, 2468,  631, 2239, 2628, 3803, 4347, 1588,  641, 1971, 3355,\n",
       "       2244, 4814,  576, 4751, 4042, 4255, 3902, 4929, 3241,  965, 2560,\n",
       "       1108, 2587,  222,   33,  859, 4710, 4309,  807, 2895, 4568, 2465,\n",
       "       3012, 3785, 1282, 1570, 2681, 1970, 3546, 2557, 2143,  621, 1791,\n",
       "       2721, 4949, 4495, 3426, 3846,  850,  617, 1197, 1737, 4129, 3726,\n",
       "       1898, 2126, 4491, 1342,  648, 4714, 4216, 2005,   75, 3618, 1506,\n",
       "       4435, 4608, 1541, 1565, 1753, 3873, 3796,  554, 2369, 2189, 4963,\n",
       "       2660, 3888, 1863, 1460,   65,  470, 3747, 2750,  469, 2725, 3996,\n",
       "        104, 1977, 3019,  327, 4981, 3194, 1091, 3671, 3864, 1185, 3716,\n",
       "       4877, 4097, 4395,  587, 3670, 3184, 2177, 2901, 4510, 2054, 1758,\n",
       "       2764, 1537, 4757, 4373, 2385,  764, 4131, 2651, 1076, 3384, 2133,\n",
       "         46, 4457, 3513, 3149, 2965, 1414,  535, 1226, 1655, 3871, 2521,\n",
       "       3259, 4403, 1841, 2704, 3022, 1746, 2483, 2316, 3128, 4136, 3361,\n",
       "       3865, 2466, 2121, 4079,  974, 3810, 2485,  556, 2738, 4523, 2257,\n",
       "       3367, 1045,  119, 2062, 1974, 3921,  620, 4113, 1718, 2022, 2277,\n",
       "        708, 1349,  364, 3879, 3602, 2260, 3159, 4147, 2344,   97, 3885,\n",
       "       2982, 2444, 4893, 1314, 2421,  993, 1396, 4842, 2167, 1232, 1721,\n",
       "       4883,  335,  955, 1544,  872, 4948, 4263,  311, 1703, 3249, 3802,\n",
       "       2009, 4076,  196, 1385,   76,  487, 2980, 1548, 2648, 3322, 3438,\n",
       "        507, 1692, 2529, 2732, 4554, 1360,  264, 1329, 4654, 1374, 2462,\n",
       "       4187, 1927, 2944,  255, 3081, 1897, 2843,  113,  265,   72, 3156,\n",
       "       4509, 1482, 4713, 2275, 3027, 1101, 4124, 3117, 3016, 2493, 3422,\n",
       "       2412, 2600, 3931, 3429, 2389, 1212, 2480, 4086,  296, 2261, 1260,\n",
       "       2933, 4707, 1402,  283, 3652, 3279, 4556, 3616, 1749, 4592, 3459,\n",
       "       3530, 3221, 3301,  803, 2921, 1910, 2281,  237,  831, 1571],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1000_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d89191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
