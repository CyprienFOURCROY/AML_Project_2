{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f7cc81",
   "metadata": {},
   "source": [
    "### a) data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ee776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4250, 500) (4250, 1)\n",
      "(750, 500) (750, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "project_path = Path.cwd()  \n",
    "\n",
    "\n",
    "data_path = project_path / \"data\"\n",
    "\n",
    "X_train = pd.read_parquet(data_path / \"X_train.parquet\")\n",
    "y_train = pd.read_parquet(data_path / \"y_train.parquet\")\n",
    "X_test = pd.read_parquet(data_path / \"X_test.parquet\")\n",
    "y_test = pd.read_parquet(data_path / \"y_test.parquet\")\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eacf841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Variables Possible:  50\n"
     ]
    }
   ],
   "source": [
    "Gain_Max_Households = 10*1000\n",
    "Max_Variables_Possibles = Gain_Max_Households // 200 \n",
    "print(\"Max Variables Possible: \", Max_Variables_Possibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a453bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_scaled_gain(\n",
    "    y_true,\n",
    "    y_pred_proba,\n",
    "    num_features,\n",
    "    gain_per_tp=10,\n",
    "    selection_ratio=0.2,\n",
    "    max_reward=10000,\n",
    "    feature_cost=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a scaled gain similar to the XGBoost approach.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth binary labels (0 or 1).\n",
    "    y_pred_proba : array-like\n",
    "        Predicted probabilities for the positive class.\n",
    "    num_features : int\n",
    "        Number of selected features used by the model.\n",
    "    gain_per_tp : int, default=10\n",
    "        Gain earned for each true positive selected.\n",
    "    selection_ratio : float, default=0.2\n",
    "        Fraction of instances to select (top K by predicted probability).\n",
    "    max_reward : int, default=10000\n",
    "        Scaling factor for maximum reward (e.g. if all top K are correct).\n",
    "    feature_cost : int, default=200\n",
    "        Cost per selected feature.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    scaled_gain : float\n",
    "        The estimated scaled gain for this configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    n_selected = int(selection_ratio * len(y_true))\n",
    "    top_k_indices = np.argsort(y_pred_proba)[-n_selected:]\n",
    "    true_positives = y_true[top_k_indices].sum()\n",
    "\n",
    "    reward = (true_positives * gain_per_tp * max_reward) / (n_selected * gain_per_tp)\n",
    "    cost = num_features * feature_cost\n",
    "\n",
    "    return reward - cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f699257",
   "metadata": {},
   "source": [
    "## First algorithm : Logistic regression features + SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0b01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def select_features_L1(X_train, y_train, C_feat):\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l1\", solver=\"liblinear\", C=C_feat, random_state=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    coefs = model.coef_.ravel()\n",
    "    selected_indices = np.where(coefs != 0)[0]\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3d51e",
   "metadata": {},
   "source": [
    "### grid-search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e383c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying kernel=rbf, C_feat = 0.0005, C_svm = 0.01\n",
      "  Fold 1: features = 1, gain = 6917.65\n",
      "  Fold 2: features = 1, gain = 7211.76\n",
      "  Fold 3: features = 1, gain = 6858.82\n",
      "  Fold 4: features = 1, gain = 7094.12\n",
      "  Fold 5: features = 1, gain = 7564.71\n",
      "Average gain for kernel=rbf, C_feat=0.0005, C_svm=0.01: 7129.41\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0005, C_svm = 0.1\n",
      "  Fold 1: features = 1, gain = 7329.41\n",
      "  Fold 2: features = 1, gain = 7211.76\n",
      "  Fold 3: features = 1, gain = 6800.00\n",
      "  Fold 4: features = 1, gain = 6917.65\n",
      "  Fold 5: features = 1, gain = 7094.12\n",
      "Average gain for kernel=rbf, C_feat=0.0005, C_svm=0.1: 7070.59\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0005, C_svm = 1.0\n",
      "  Fold 1: features = 1, gain = 7329.41\n",
      "  Fold 2: features = 1, gain = 7682.35\n",
      "  Fold 3: features = 1, gain = 6858.82\n",
      "  Fold 4: features = 1, gain = 6917.65\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=rbf, C_feat=0.0005, C_svm=1.0: 7247.06\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0005, C_svm = 10.0\n",
      "  Fold 1: features = 1, gain = 7388.24\n",
      "  Fold 2: features = 1, gain = 7564.71\n",
      "  Fold 3: features = 1, gain = 6800.00\n",
      "  Fold 4: features = 1, gain = 7035.29\n",
      "  Fold 5: features = 1, gain = 7505.88\n",
      "Average gain for kernel=rbf, C_feat=0.0005, C_svm=10.0: 7258.82\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0007, C_svm = 0.01\n",
      "  Fold 1: features = 1, gain = 6917.65\n",
      "  Fold 2: features = 1, gain = 7211.76\n",
      "  Fold 3: features = 2, gain = 6482.35\n",
      "  Fold 4: features = 1, gain = 7094.12\n",
      "  Fold 5: features = 1, gain = 7564.71\n",
      "Average gain for kernel=rbf, C_feat=0.0007, C_svm=0.01: 7054.12\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0007, C_svm = 0.1\n",
      "  Fold 1: features = 1, gain = 7329.41\n",
      "  Fold 2: features = 1, gain = 7211.76\n",
      "  Fold 3: features = 2, gain = 6600.00\n",
      "  Fold 4: features = 1, gain = 6917.65\n",
      "  Fold 5: features = 1, gain = 7094.12\n",
      "Average gain for kernel=rbf, C_feat=0.0007, C_svm=0.1: 7030.59\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0007, C_svm = 1.0\n",
      "  Fold 1: features = 1, gain = 7329.41\n",
      "  Fold 2: features = 1, gain = 7682.35\n",
      "  Fold 3: features = 2, gain = 6423.53\n",
      "  Fold 4: features = 1, gain = 6917.65\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=rbf, C_feat=0.0007, C_svm=1.0: 7160.00\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0007, C_svm = 10.0\n",
      "  Fold 1: features = 1, gain = 7388.24\n",
      "  Fold 2: features = 1, gain = 7564.71\n",
      "  Fold 3: features = 2, gain = 6482.35\n",
      "  Fold 4: features = 1, gain = 7035.29\n",
      "  Fold 5: features = 1, gain = 7505.88\n",
      "Average gain for kernel=rbf, C_feat=0.0007, C_svm=10.0: 7195.29\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0009, C_svm = 0.01\n",
      "  Fold 1: features = 9, gain = 5200.00\n",
      "  Fold 2: features = 12, gain = 4835.29\n",
      "  Fold 3: features = 11, gain = 4623.53\n",
      "  Fold 4: features = 9, gain = 5258.82\n",
      "  Fold 5: features = 8, gain = 5811.76\n",
      "Average gain for kernel=rbf, C_feat=0.0009, C_svm=0.01: 5145.88\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0009, C_svm = 0.1\n",
      "  Fold 1: features = 9, gain = 5141.18\n",
      "  Fold 2: features = 12, gain = 4952.94\n",
      "  Fold 3: features = 11, gain = 4564.71\n",
      "  Fold 4: features = 9, gain = 5258.82\n",
      "  Fold 5: features = 8, gain = 6105.88\n",
      "Average gain for kernel=rbf, C_feat=0.0009, C_svm=0.1: 5204.71\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0009, C_svm = 1.0\n",
      "  Fold 1: features = 9, gain = 5258.82\n",
      "  Fold 2: features = 12, gain = 4894.12\n",
      "  Fold 3: features = 11, gain = 4741.18\n",
      "  Fold 4: features = 9, gain = 5317.65\n",
      "  Fold 5: features = 8, gain = 5752.94\n",
      "Average gain for kernel=rbf, C_feat=0.0009, C_svm=1.0: 5192.94\n",
      "\n",
      "Trying kernel=rbf, C_feat = 0.0009, C_svm = 10.0\n",
      "  Fold 1: features = 9, gain = 5082.35\n",
      "  Fold 2: features = 12, gain = 4894.12\n",
      "  Fold 3: features = 11, gain = 4800.00\n",
      "  Fold 4: features = 9, gain = 5258.82\n",
      "  Fold 5: features = 8, gain = 5400.00\n",
      "Average gain for kernel=rbf, C_feat=0.0009, C_svm=10.0: 5087.06\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0005, C_svm = 0.01\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6917.65\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0005, C_svm=0.01: 7047.06\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0005, C_svm = 0.1\n",
      "  Fold 1: features = 1, gain = 6682.35\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6917.65\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0005, C_svm=0.1: 7023.53\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0005, C_svm = 1.0\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6976.47\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0005, C_svm=1.0: 7058.82\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0005, C_svm = 10.0\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6976.47\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0005, C_svm=10.0: 7058.82\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0007, C_svm = 0.01\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 2, gain = 6835.29\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0007, C_svm=0.01: 7030.59\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0007, C_svm = 0.1\n",
      "  Fold 1: features = 1, gain = 6682.35\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 2, gain = 6776.47\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0007, C_svm=0.1: 6995.29\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0007, C_svm = 1.0\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 2, gain = 6835.29\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0007, C_svm=1.0: 7030.59\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0007, C_svm = 10.0\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 2, gain = 6835.29\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, C_feat=0.0007, C_svm=10.0: 7030.59\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0009, C_svm = 0.01\n",
      "  Fold 1: features = 9, gain = 5376.47\n",
      "  Fold 2: features = 12, gain = 5188.24\n",
      "  Fold 3: features = 11, gain = 4858.82\n",
      "  Fold 4: features = 9, gain = 5435.29\n",
      "  Fold 5: features = 8, gain = 6047.06\n",
      "Average gain for kernel=poly, C_feat=0.0009, C_svm=0.01: 5381.18\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0009, C_svm = 0.1\n",
      "  Fold 1: features = 9, gain = 5552.94\n",
      "  Fold 2: features = 12, gain = 5070.59\n",
      "  Fold 3: features = 11, gain = 5152.94\n",
      "  Fold 4: features = 9, gain = 5552.94\n",
      "  Fold 5: features = 8, gain = 5870.59\n",
      "Average gain for kernel=poly, C_feat=0.0009, C_svm=0.1: 5440.00\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0009, C_svm = 1.0\n",
      "  Fold 1: features = 9, gain = 5376.47\n",
      "  Fold 2: features = 12, gain = 4776.47\n",
      "  Fold 3: features = 11, gain = 5035.29\n",
      "  Fold 4: features = 9, gain = 5494.12\n",
      "  Fold 5: features = 8, gain = 5635.29\n",
      "Average gain for kernel=poly, C_feat=0.0009, C_svm=1.0: 5263.53\n",
      "\n",
      "Trying kernel=poly, C_feat = 0.0009, C_svm = 10.0\n",
      "  Fold 1: features = 9, gain = 5258.82\n",
      "  Fold 2: features = 12, gain = 4541.18\n",
      "  Fold 3: features = 11, gain = 4917.65\n",
      "  Fold 4: features = 9, gain = 5435.29\n",
      "  Fold 5: features = 8, gain = 5694.12\n",
      "Average gain for kernel=poly, C_feat=0.0009, C_svm=10.0: 5169.41\n",
      "\n",
      "Best configuration: kernel=rbf, C_feat=0.0005, C_svm=10.0 with gain: 7258.82\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def compute_scaled_gain(\n",
    "    y_true,\n",
    "    y_pred_proba,\n",
    "    num_features,\n",
    "    gain_per_tp=10,\n",
    "    selection_ratio=0.2,\n",
    "    max_reward=10000,\n",
    "    feature_cost=200,\n",
    "):\n",
    "    n_selected = int(selection_ratio * len(y_true))\n",
    "    top_k_indices = np.argsort(y_pred_proba)[-n_selected:]\n",
    "    true_positives = np.array(y_true)[top_k_indices].sum()\n",
    "    reward = (true_positives * gain_per_tp * max_reward) / (n_selected * gain_per_tp)\n",
    "    cost = num_features * feature_cost\n",
    "    return reward - cost\n",
    "\n",
    "\n",
    "\n",
    "C_feat_grid = [0.0005,  0.0007,  0.0009]\n",
    "C_svm_grid = [ 0.01, 0.1, 1.0, 10.0]\n",
    "kernel_options = [\"rbf\", \"poly\"]\n",
    "\n",
    "\n",
    "best_config = None\n",
    "best_gain = -np.inf\n",
    "\n",
    "results = []\n",
    "\n",
    "for kernel in kernel_options:\n",
    "    for C_feat in C_feat_grid:\n",
    "        for C_svm in C_svm_grid:\n",
    "            print(f\"\\nTrying kernel={kernel}, C_feat = {C_feat}, C_svm = {C_svm}\")\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            total_gain = 0\n",
    "\n",
    "            for index, (train_idx, val_idx) in enumerate(\n",
    "                skf.split(X_train, y_train), 1\n",
    "            ):\n",
    "                X_inner, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_inner, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                selected_features = select_features_L1(\n",
    "                    X_inner.values, y_inner.values.ravel(), C_feat\n",
    "                )\n",
    "                num_selected = len(selected_features)\n",
    "\n",
    "                if num_selected == 0:\n",
    "                    print(f\"  Fold {index}: no features selected, skipping\")\n",
    "                    continue\n",
    "\n",
    "                model = make_pipeline(\n",
    "                    StandardScaler(),\n",
    "                    svm.SVC(kernel=kernel, C=C_svm, probability=True, random_state=42),\n",
    "                )\n",
    "                model.fit(X_inner.iloc[:, selected_features], y_inner.values.ravel())\n",
    "                y_proba_val = model.predict_proba(X_val.iloc[:, selected_features])[\n",
    "                    :, 1\n",
    "                ]\n",
    "\n",
    "                gain = compute_scaled_gain(\n",
    "                    y_val, y_proba_val, num_features=num_selected\n",
    "                )\n",
    "                print(f\"  Fold {index}: features = {num_selected}, gain = {gain:.2f}\")\n",
    "                total_gain += gain\n",
    "\n",
    "            avg_gain = total_gain / skf.get_n_splits()\n",
    "            print(\n",
    "                f\"Average gain for kernel={kernel}, C_feat={C_feat}, C_svm={C_svm}: {avg_gain:.2f}\"\n",
    "            )\n",
    "\n",
    "            results.append((kernel, C_feat, C_svm, avg_gain))\n",
    "\n",
    "            if avg_gain > best_gain:\n",
    "                best_gain = avg_gain\n",
    "                best_config = (kernel, C_feat, C_svm)\n",
    "\n",
    "best_kernel, best_C_feat, best_C_svm = best_config\n",
    "print(\n",
    "    f\"\\nBest configuration: kernel={best_kernel}, C_feat={best_C_feat}, C_svm={best_C_svm} with gain: {best_gain:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aed044",
   "metadata": {},
   "source": [
    "### Evaluation on the X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Scaled Gain on Test Set: 7066.67\n",
      "Accuracy: 0.7213\n",
      "Precision: 0.7105\n",
      "F1 Score: 0.7172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "best_kernel, best_C_feat, best_C_svm = best_config\n",
    "\n",
    "final_selected = select_features_L1(X_train.values, y_train.values.ravel(), best_C_feat)\n",
    "\n",
    "final_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    svm.SVC(kernel=best_kernel, C=best_C_svm, probability=True, random_state=42),\n",
    ")\n",
    "final_model.fit(X_train.iloc[:, final_selected], y_train.values.ravel())\n",
    "\n",
    "y_proba_test = final_model.predict_proba(X_test.iloc[:, final_selected])[:, 1]\n",
    "y_pred_test = final_model.predict(X_test.iloc[:, final_selected])\n",
    "\n",
    "scaled_gain_test = compute_scaled_gain(\n",
    "    y_test, y_proba_test, num_features=len(final_selected)\n",
    ")\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Final Scaled Gain on Test Set: {scaled_gain_test:.2f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932a89e",
   "metadata": {},
   "source": [
    "## Second algorithm : Mutual Information + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25ab419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_features_MI(X_train, y_train, threshold):\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_train, y_train, discrete_features=False, random_state=0\n",
    "    )\n",
    "    selected_indices = [i for i, score in enumerate(mi_scores) if score > threshold]\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a6369",
   "metadata": {},
   "source": [
    "### grid search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe3c63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying kernel=rbf, MI threshold = 0.06, C_svm = 0.01\n",
      "  Fold 1: features = 4, gain = 6435.29\n",
      "  Fold 2: features = 3, gain = 7223.53\n",
      "  Fold 3: features = 4, gain = 6435.29\n",
      "  Fold 4: features = 4, gain = 6494.12\n",
      "  Fold 5: features = 4, gain = 6670.59\n",
      "Average gain for kernel=rbf, MI threshold=0.06, C_svm=0.01: 6651.76\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.06, C_svm = 0.1\n",
      "  Fold 1: features = 4, gain = 6435.29\n",
      "  Fold 2: features = 3, gain = 6870.59\n",
      "  Fold 3: features = 4, gain = 6376.47\n",
      "  Fold 4: features = 4, gain = 6611.76\n",
      "  Fold 5: features = 4, gain = 6788.24\n",
      "Average gain for kernel=rbf, MI threshold=0.06, C_svm=0.1: 6616.47\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.06, C_svm = 1.0\n",
      "  Fold 1: features = 4, gain = 6670.59\n",
      "  Fold 2: features = 3, gain = 6870.59\n",
      "  Fold 3: features = 4, gain = 6317.65\n",
      "  Fold 4: features = 4, gain = 6552.94\n",
      "  Fold 5: features = 4, gain = 6552.94\n",
      "Average gain for kernel=rbf, MI threshold=0.06, C_svm=1.0: 6592.94\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.06, C_svm = 10.0\n",
      "  Fold 1: features = 4, gain = 6494.12\n",
      "  Fold 2: features = 3, gain = 7105.88\n",
      "  Fold 3: features = 4, gain = 6317.65\n",
      "  Fold 4: features = 4, gain = 6611.76\n",
      "  Fold 5: features = 4, gain = 6905.88\n",
      "Average gain for kernel=rbf, MI threshold=0.06, C_svm=10.0: 6687.06\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.07, C_svm = 0.01\n",
      "  Fold 1: features = 2, gain = 6776.47\n",
      "  Fold 2: features = 2, gain = 7247.06\n",
      "  Fold 3: features = 2, gain = 6835.29\n",
      "  Fold 4: features = 1, gain = 7094.12\n",
      "  Fold 5: features = 2, gain = 7247.06\n",
      "Average gain for kernel=rbf, MI threshold=0.07, C_svm=0.01: 7040.00\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.07, C_svm = 0.1\n",
      "  Fold 1: features = 2, gain = 7070.59\n",
      "  Fold 2: features = 2, gain = 6835.29\n",
      "  Fold 3: features = 2, gain = 7011.76\n",
      "  Fold 4: features = 1, gain = 6917.65\n",
      "  Fold 5: features = 2, gain = 7011.76\n",
      "Average gain for kernel=rbf, MI threshold=0.07, C_svm=0.1: 6969.41\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.07, C_svm = 1.0\n",
      "  Fold 1: features = 2, gain = 7305.88\n",
      "  Fold 2: features = 2, gain = 7247.06\n",
      "  Fold 3: features = 2, gain = 6952.94\n",
      "  Fold 4: features = 1, gain = 6917.65\n",
      "  Fold 5: features = 2, gain = 7070.59\n",
      "Average gain for kernel=rbf, MI threshold=0.07, C_svm=1.0: 7098.82\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.07, C_svm = 10.0\n",
      "  Fold 1: features = 2, gain = 6952.94\n",
      "  Fold 2: features = 2, gain = 7423.53\n",
      "  Fold 3: features = 2, gain = 6894.12\n",
      "  Fold 4: features = 1, gain = 7035.29\n",
      "  Fold 5: features = 2, gain = 7305.88\n",
      "Average gain for kernel=rbf, MI threshold=0.07, C_svm=10.0: 7122.35\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.08, C_svm = 0.01\n",
      "  Fold 1: features = 1, gain = 6917.65\n",
      "  Fold 2: features = 1, gain = 7211.76\n",
      "  Fold 3: features = 1, gain = 6858.82\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7564.71\n",
      "Average gain for kernel=rbf, MI threshold=0.08, C_svm=0.01: 5710.59\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.08, C_svm = 0.1\n",
      "  Fold 1: features = 1, gain = 7329.41\n",
      "  Fold 2: features = 1, gain = 7211.76\n",
      "  Fold 3: features = 1, gain = 6800.00\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7094.12\n",
      "Average gain for kernel=rbf, MI threshold=0.08, C_svm=0.1: 5687.06\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.08, C_svm = 1.0\n",
      "  Fold 1: features = 1, gain = 7329.41\n",
      "  Fold 2: features = 1, gain = 7682.35\n",
      "  Fold 3: features = 1, gain = 6858.82\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=rbf, MI threshold=0.08, C_svm=1.0: 5863.53\n",
      "\n",
      "Trying kernel=rbf, MI threshold = 0.08, C_svm = 10.0\n",
      "  Fold 1: features = 1, gain = 7388.24\n",
      "  Fold 2: features = 1, gain = 7564.71\n",
      "  Fold 3: features = 1, gain = 6800.00\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7505.88\n",
      "Average gain for kernel=rbf, MI threshold=0.08, C_svm=10.0: 5851.76\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.06, C_svm = 0.01\n",
      "  Fold 1: features = 4, gain = 6141.18\n",
      "  Fold 2: features = 3, gain = 6988.24\n",
      "  Fold 3: features = 4, gain = 6317.65\n",
      "  Fold 4: features = 4, gain = 6435.29\n",
      "  Fold 5: features = 4, gain = 6729.41\n",
      "Average gain for kernel=poly, MI threshold=0.06, C_svm=0.01: 6522.35\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.06, C_svm = 0.1\n",
      "  Fold 1: features = 4, gain = 6141.18\n",
      "  Fold 2: features = 3, gain = 6752.94\n",
      "  Fold 3: features = 4, gain = 6200.00\n",
      "  Fold 4: features = 4, gain = 6494.12\n",
      "  Fold 5: features = 4, gain = 7023.53\n",
      "Average gain for kernel=poly, MI threshold=0.06, C_svm=0.1: 6522.35\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.06, C_svm = 1.0\n",
      "  Fold 1: features = 4, gain = 6141.18\n",
      "  Fold 2: features = 3, gain = 6576.47\n",
      "  Fold 3: features = 4, gain = 6435.29\n",
      "  Fold 4: features = 4, gain = 6552.94\n",
      "  Fold 5: features = 4, gain = 6729.41\n",
      "Average gain for kernel=poly, MI threshold=0.06, C_svm=1.0: 6487.06\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.06, C_svm = 10.0\n",
      "  Fold 1: features = 4, gain = 6141.18\n",
      "  Fold 2: features = 3, gain = 6517.65\n",
      "  Fold 3: features = 4, gain = 6435.29\n",
      "  Fold 4: features = 4, gain = 6788.24\n",
      "  Fold 5: features = 4, gain = 6788.24\n",
      "Average gain for kernel=poly, MI threshold=0.06, C_svm=10.0: 6534.12\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.07, C_svm = 0.01\n",
      "  Fold 1: features = 2, gain = 6541.18\n",
      "  Fold 2: features = 2, gain = 7188.24\n",
      "  Fold 3: features = 2, gain = 6600.00\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 2, gain = 7247.06\n",
      "Average gain for kernel=poly, MI threshold=0.07, C_svm=0.01: 6910.59\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.07, C_svm = 0.1\n",
      "  Fold 1: features = 2, gain = 6658.82\n",
      "  Fold 2: features = 2, gain = 7070.59\n",
      "  Fold 3: features = 2, gain = 6717.65\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 2, gain = 7305.88\n",
      "Average gain for kernel=poly, MI threshold=0.07, C_svm=0.1: 6945.88\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.07, C_svm = 1.0\n",
      "  Fold 1: features = 2, gain = 6541.18\n",
      "  Fold 2: features = 2, gain = 6835.29\n",
      "  Fold 3: features = 2, gain = 6658.82\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 2, gain = 7305.88\n",
      "Average gain for kernel=poly, MI threshold=0.07, C_svm=1.0: 6863.53\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.07, C_svm = 10.0\n",
      "  Fold 1: features = 2, gain = 6600.00\n",
      "  Fold 2: features = 2, gain = 6894.12\n",
      "  Fold 3: features = 2, gain = 6600.00\n",
      "  Fold 4: features = 1, gain = 6976.47\n",
      "  Fold 5: features = 2, gain = 7423.53\n",
      "Average gain for kernel=poly, MI threshold=0.07, C_svm=10.0: 6898.82\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.08, C_svm = 0.01\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6917.65\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, MI threshold=0.08, C_svm=0.01: 5651.76\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.08, C_svm = 0.1\n",
      "  Fold 1: features = 1, gain = 6682.35\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6917.65\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, MI threshold=0.08, C_svm=0.1: 5628.24\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.08, C_svm = 1.0\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6976.47\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, MI threshold=0.08, C_svm=1.0: 5663.53\n",
      "\n",
      "Trying kernel=poly, MI threshold = 0.08, C_svm = 10.0\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 7094.12\n",
      "  Fold 3: features = 1, gain = 6976.47\n",
      "  Fold 4: no features selected, skipping\n",
      "  Fold 5: features = 1, gain = 7447.06\n",
      "Average gain for kernel=poly, MI threshold=0.08, C_svm=10.0: 5663.53\n",
      "\n",
      "Best configuration: kernel=rbf, MI threshold=0.07, C_svm=10.0 with gain: 7122.35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "def compute_scaled_gain(\n",
    "    y_true,\n",
    "    y_pred_proba,\n",
    "    num_features,\n",
    "    gain_per_tp=10,\n",
    "    selection_ratio=0.2,\n",
    "    max_reward=10000,\n",
    "    feature_cost=200,\n",
    "):\n",
    "    n_selected = int(selection_ratio * len(y_true))\n",
    "    top_k_indices = np.argsort(y_pred_proba)[-n_selected:]\n",
    "    true_positives = np.array(y_true)[top_k_indices].sum()\n",
    "    reward = (true_positives * gain_per_tp * max_reward) / (n_selected * gain_per_tp)\n",
    "    cost = num_features * feature_cost\n",
    "    return reward - cost\n",
    "\n",
    "\n",
    "mi_thresholds = [0.06, 0.07, 0.08]\n",
    "C_svm_grid = [0.01, 0.1, 1.0, 10.0]\n",
    "kernel_options = [\"rbf\", \"poly\"]\n",
    "\n",
    "\n",
    "best_config = None\n",
    "best_gain = -np.inf\n",
    "results = []\n",
    "\n",
    "for kernel in kernel_options:\n",
    "    for C_feat in mi_thresholds:\n",
    "        for C_svm in C_svm_grid:\n",
    "            print(f\"\\nTrying kernel={kernel}, MI threshold = {C_feat}, C_svm = {C_svm}\")\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            total_gain = 0\n",
    "\n",
    "            for index, (train_idx, val_idx) in enumerate(\n",
    "                skf.split(X_train, y_train), 1\n",
    "            ):\n",
    "                X_inner, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_inner, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                selected_features = select_features_MI(\n",
    "                    X_inner.values, y_inner.values.ravel(), C_feat\n",
    "                )\n",
    "                num_selected = len(selected_features)\n",
    "\n",
    "                if num_selected == 0:\n",
    "                    print(f\"  Fold {index}: no features selected, skipping\")\n",
    "                    continue\n",
    "\n",
    "                model = make_pipeline(\n",
    "                    StandardScaler(),\n",
    "                    svm.SVC(kernel=kernel, C=C_svm, probability=True, random_state=42),\n",
    "                )\n",
    "                model.fit(X_inner.iloc[:, selected_features], y_inner.values.ravel())\n",
    "                y_proba_val = model.predict_proba(X_val.iloc[:, selected_features])[\n",
    "                    :, 1\n",
    "                ]\n",
    "\n",
    "                gain = compute_scaled_gain(\n",
    "                    y_val, y_proba_val, num_features=num_selected\n",
    "                )\n",
    "                print(f\"  Fold {index}: features = {num_selected}, gain = {gain:.2f}\")\n",
    "                total_gain += gain\n",
    "\n",
    "            avg_gain = total_gain / skf.get_n_splits()\n",
    "            print(\n",
    "                f\"Average gain for kernel={kernel}, MI threshold={C_feat}, C_svm={C_svm}: {avg_gain:.2f}\"\n",
    "            )\n",
    "\n",
    "            results.append((kernel, C_feat, C_svm, avg_gain))\n",
    "\n",
    "            if avg_gain > best_gain:\n",
    "                best_gain = avg_gain\n",
    "                best_config = (kernel, C_feat, C_svm)\n",
    "\n",
    "best_kernel, best_C_feat, best_C_svm = best_config\n",
    "print(\n",
    "    f\"\\nBest configuration: kernel={best_kernel}, MI threshold={best_C_feat}, C_svm={best_C_svm} with gain: {best_gain:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7204d1",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Scaled Gain on Test Set: 6866.67\n",
      "Accuracy: 0.7320\n",
      "Precision: 0.7188\n",
      "F1 Score: 0.7295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "best_kernel, best_C_feat, best_C_svm = best_config\n",
    "\n",
    "\n",
    "final_selected = select_features_MI(X_train.values, y_train.values.ravel(), best_C_feat)\n",
    "\n",
    "final_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    svm.SVC(kernel=best_kernel, C=best_C_svm, probability=True, random_state=42),\n",
    ")\n",
    "final_model.fit(X_train.iloc[:, final_selected], y_train.values.ravel())\n",
    "\n",
    "y_proba_test = final_model.predict_proba(X_test.iloc[:, final_selected])[:, 1]\n",
    "y_pred_test = final_model.predict(X_test.iloc[:, final_selected])\n",
    "\n",
    "scaled_gain_test = compute_scaled_gain(\n",
    "    y_test, y_proba_test, num_features=len(final_selected)\n",
    ")\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Final Scaled Gain on Test Set: {scaled_gain_test:.2f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43093229",
   "metadata": {},
   "source": [
    "## Third algorithm : Mutual Information + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_features_MI(X, y, threshold=0.01):\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X, y, discrete_features=False, random_state=0, n_neighbors=5\n",
    "    )\n",
    "    return [i for i, score in enumerate(mi_scores) if score > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c53924",
   "metadata": {},
   "source": [
    "### Grid-search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7394a161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying MI threshold=0.06, n_estimators=50, max_depth=5\n",
      "  Fold 1: features = 6, gain = 5800.00\n",
      "  Fold 2: features = 5, gain = 6588.24\n",
      "  Fold 3: features = 6, gain = 6211.76\n",
      "  Fold 4: features = 4, gain = 6494.12\n",
      "  Fold 5: features = 5, gain = 6470.59\n",
      "Average gain for threshold=0.06, n_estimators=50, max_depth=5: 6312.94\n",
      "\n",
      "Trying MI threshold=0.06, n_estimators=50, max_depth=None\n",
      "  Fold 1: features = 6, gain = 5564.71\n",
      "  Fold 2: features = 5, gain = 6176.47\n",
      "  Fold 3: features = 6, gain = 5976.47\n",
      "  Fold 4: features = 4, gain = 6552.94\n",
      "  Fold 5: features = 5, gain = 6470.59\n",
      "Average gain for threshold=0.06, n_estimators=50, max_depth=None: 6148.24\n",
      "\n",
      "Trying MI threshold=0.06, n_estimators=100, max_depth=5\n",
      "  Fold 1: features = 6, gain = 5976.47\n",
      "  Fold 2: features = 5, gain = 6352.94\n",
      "  Fold 3: features = 6, gain = 6094.12\n",
      "  Fold 4: features = 4, gain = 6494.12\n",
      "  Fold 5: features = 5, gain = 6470.59\n",
      "Average gain for threshold=0.06, n_estimators=100, max_depth=5: 6277.65\n",
      "\n",
      "Trying MI threshold=0.06, n_estimators=100, max_depth=None\n",
      "  Fold 1: features = 6, gain = 5682.35\n",
      "  Fold 2: features = 5, gain = 5823.53\n",
      "  Fold 3: features = 6, gain = 5858.82\n",
      "  Fold 4: features = 4, gain = 6611.76\n",
      "  Fold 5: features = 5, gain = 6117.65\n",
      "Average gain for threshold=0.06, n_estimators=100, max_depth=None: 6018.82\n",
      "\n",
      "Trying MI threshold=0.07, n_estimators=50, max_depth=5\n",
      "  Fold 1: features = 2, gain = 6482.35\n",
      "  Fold 2: features = 1, gain = 7447.06\n",
      "  Fold 3: features = 2, gain = 6894.12\n",
      "  Fold 4: features = 2, gain = 7541.18\n",
      "  Fold 5: features = 3, gain = 6929.41\n",
      "Average gain for threshold=0.07, n_estimators=50, max_depth=5: 7058.82\n",
      "\n",
      "Trying MI threshold=0.07, n_estimators=50, max_depth=None\n",
      "  Fold 1: features = 2, gain = 6070.59\n",
      "  Fold 2: features = 1, gain = 5976.47\n",
      "  Fold 3: features = 2, gain = 6482.35\n",
      "  Fold 4: features = 2, gain = 7070.59\n",
      "  Fold 5: features = 3, gain = 6635.29\n",
      "Average gain for threshold=0.07, n_estimators=50, max_depth=None: 6447.06\n",
      "\n",
      "Trying MI threshold=0.07, n_estimators=100, max_depth=5\n",
      "  Fold 1: features = 2, gain = 6541.18\n",
      "  Fold 2: features = 1, gain = 7329.41\n",
      "  Fold 3: features = 2, gain = 6894.12\n",
      "  Fold 4: features = 2, gain = 7541.18\n",
      "  Fold 5: features = 3, gain = 6811.76\n",
      "Average gain for threshold=0.07, n_estimators=100, max_depth=5: 7023.53\n",
      "\n",
      "Trying MI threshold=0.07, n_estimators=100, max_depth=None\n",
      "  Fold 1: features = 2, gain = 5952.94\n",
      "  Fold 2: features = 1, gain = 6094.12\n",
      "  Fold 3: features = 2, gain = 6658.82\n",
      "  Fold 4: features = 2, gain = 7129.41\n",
      "  Fold 5: features = 3, gain = 6929.41\n",
      "Average gain for threshold=0.07, n_estimators=100, max_depth=None: 6552.94\n",
      "\n",
      "Trying MI threshold=0.08, n_estimators=50, max_depth=5\n",
      "  Fold 1: features = 1, gain = 7152.94\n",
      "  Fold 2: features = 1, gain = 7447.06\n",
      "  Fold 3: features = 1, gain = 6917.65\n",
      "  Fold 4: features = 1, gain = 7447.06\n",
      "  Fold 5: features = 1, gain = 7211.76\n",
      "Average gain for threshold=0.08, n_estimators=50, max_depth=5: 7235.29\n",
      "\n",
      "Trying MI threshold=0.08, n_estimators=50, max_depth=None\n",
      "  Fold 1: features = 1, gain = 6858.82\n",
      "  Fold 2: features = 1, gain = 5976.47\n",
      "  Fold 3: features = 1, gain = 6564.71\n",
      "  Fold 4: features = 1, gain = 6505.88\n",
      "  Fold 5: features = 1, gain = 6564.71\n",
      "Average gain for threshold=0.08, n_estimators=50, max_depth=None: 6494.12\n",
      "\n",
      "Trying MI threshold=0.08, n_estimators=100, max_depth=5\n",
      "  Fold 1: features = 1, gain = 7035.29\n",
      "  Fold 2: features = 1, gain = 7329.41\n",
      "  Fold 3: features = 1, gain = 6858.82\n",
      "  Fold 4: features = 1, gain = 7329.41\n",
      "  Fold 5: features = 1, gain = 7211.76\n",
      "Average gain for threshold=0.08, n_estimators=100, max_depth=5: 7152.94\n",
      "\n",
      "Trying MI threshold=0.08, n_estimators=100, max_depth=None\n",
      "  Fold 1: features = 1, gain = 6800.00\n",
      "  Fold 2: features = 1, gain = 6094.12\n",
      "  Fold 3: features = 1, gain = 6564.71\n",
      "  Fold 4: features = 1, gain = 6447.06\n",
      "  Fold 5: features = 1, gain = 6564.71\n",
      "Average gain for threshold=0.08, n_estimators=100, max_depth=None: 6494.12\n",
      "\n",
      "Best configuration: threshold=0.08, n_estimators=50, max_depth=5 with gain: 7235.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def select_features_MI(X, y, threshold=0.01):\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X, y, discrete_features=False, random_state=0, n_neighbors=5\n",
    "    )\n",
    "    return [i for i, score in enumerate(mi_scores) if score > threshold]\n",
    "\n",
    "\n",
    "mi_thresholds = [0.06, 0.07, 0.08]\n",
    "rf_n_estimators = [50, 100]\n",
    "rf_max_depths = [5, None]\n",
    "\n",
    "best_gain = -np.inf\n",
    "best_config = None\n",
    "results = []\n",
    "\n",
    "for threshold in mi_thresholds:\n",
    "    for n_estimators in rf_n_estimators:\n",
    "        for max_depth in rf_max_depths:\n",
    "            print(\n",
    "                f\"\\nTrying MI threshold={threshold}, n_estimators={n_estimators}, max_depth={max_depth}\"\n",
    "            )\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            total_gain = 0\n",
    "\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(\n",
    "                skf.split(X_train, y_train), 1\n",
    "            ):\n",
    "                X_inner, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_inner, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                selected_features = select_features_MI(\n",
    "                    X_inner.values, y_inner.values.ravel(), threshold\n",
    "                )\n",
    "                num_selected = len(selected_features)\n",
    "\n",
    "                if num_selected == 0:\n",
    "                    print(f\"  Fold {fold_idx}: no features selected, skipping\")\n",
    "                    continue\n",
    "\n",
    "                model = make_pipeline(\n",
    "                    StandardScaler(),\n",
    "                    RandomForestClassifier(\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1,\n",
    "                    ),\n",
    "                )\n",
    "                model.fit(X_inner.iloc[:, selected_features], y_inner.values.ravel())\n",
    "                y_proba_val = model.predict_proba(X_val.iloc[:, selected_features])[\n",
    "                    :, 1\n",
    "                ]\n",
    "\n",
    "                gain = compute_scaled_gain(\n",
    "                    y_val, y_proba_val, num_features=num_selected\n",
    "                )\n",
    "                print(\n",
    "                    f\"  Fold {fold_idx}: features = {num_selected}, gain = {gain:.2f}\"\n",
    "                )\n",
    "                total_gain += gain\n",
    "\n",
    "            avg_gain = total_gain / skf.get_n_splits()\n",
    "            print(\n",
    "                f\"Average gain for threshold={threshold}, n_estimators={n_estimators}, max_depth={max_depth}: {avg_gain:.2f}\"\n",
    "            )\n",
    "\n",
    "            results.append((threshold, n_estimators, max_depth, avg_gain))\n",
    "\n",
    "            if avg_gain > best_gain:\n",
    "                best_gain = avg_gain\n",
    "                best_config = (threshold, n_estimators, max_depth)\n",
    "\n",
    "print(\n",
    "    f\"\\nBest configuration: threshold={best_config[0]}, n_estimators={best_config[1]}, max_depth={best_config[2]} with gain: {best_gain:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d00564",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc3a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set scaled gain: 7133.33\n",
      "Accuracy: 0.7253\n",
      "Precision: 0.7128\n",
      "F1 Score: 0.7224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "threshold, n_estimators, max_depth = best_config\n",
    "\n",
    "selected_features = select_features_MI(\n",
    "    X_train.values, y_train.values.ravel(), threshold\n",
    ")\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    ")\n",
    "model.fit(X_train.iloc[:, selected_features], y_train.values.ravel())\n",
    "\n",
    "y_proba_test = model.predict_proba(X_test.iloc[:, selected_features])[:, 1]\n",
    "y_pred_test = model.predict(X_test.iloc[:, selected_features])\n",
    "\n",
    "test_gain = compute_scaled_gain(\n",
    "    y_test, y_proba_test, num_features=len(selected_features)\n",
    ")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test set scaled gain: {test_gain:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d89191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
