{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67211492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from sklearn import model_selection\n",
    "import xgboost as xgb\n",
    "\n",
    "project_path = pathlib.Path('/home/mpuscian/Desktop/repozytoria/MINI_projects/AML_Project_2')\n",
    "data_path = project_path.joinpath('data')\n",
    "\n",
    "X_train_path = data_path.joinpath(\"X_train.parquet\")\n",
    "y_train_path = data_path.joinpath(\"y_train.parquet\")\n",
    "\n",
    "X_test_path = data_path.joinpath(\"X_test.parquet\")\n",
    "y_test_path = data_path.joinpath(\"y_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7ff030",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet(X_train_path)\n",
    "y_train = pd.read_parquet(y_train_path)\n",
    "\n",
    "X_test = pd.read_parquet(X_test_path)\n",
    "y_test = pd.read_parquet(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bafdec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_parquet(project_path.joinpath('data/X_train.parquet'))\n",
    "# y_train.to_parquet(project_path.joinpath('data/y_train.parquet'))\n",
    "# X_test.to_parquet(project_path.joinpath('data/X_test.parquet'))\n",
    "# y_test.to_parquet(project_path.joinpath('data/y_test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euros_gained(y_true, y_pred_proba, num_of_features : int):\n",
    "    \"\"\"\n",
    "    Metric used in this project specific that it can be used in xgboost.cv\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    y_pred\n",
    "        predictions of the model\n",
    "    data\n",
    "        X data of type xgb.DMatrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of length two: [metric : str, value : numeric scalar] \n",
    "    \"\"\"\n",
    "    n_selected = int(1/5 * len(y_true))\n",
    "    positive_class_probas = y_pred_proba[:, 1]\n",
    "\n",
    "    # Choosing 20% of households\n",
    "    top_k_indices = np.argsort(positive_class_probas)[-n_selected:]\n",
    "    true_positives = y_true[top_k_indices].sum()\n",
    "    \n",
    "    max_reward = n_selected * 10\n",
    "    reward = true_positives * 10 * 10000/max_reward\n",
    "    cost = num_of_features * 200 # 200 euros for each feature\n",
    "    return 'Euros_gained',- (reward - cost)\n",
    "\n",
    "def xgb_euros_gained(y_pred : np.ndarray, data : xgb.DMatrix):\n",
    "    \"\"\"\n",
    "    Metric used in this project specific that it can be used in xgboost.cv\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    y_pred\n",
    "        predictions of the model\n",
    "    data\n",
    "        X data of type xgb.DMatrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of length two: [metric : str, value : numeric scalar] \n",
    "    \"\"\"\n",
    "    y_true = data.get_label()\n",
    "    \n",
    "    n_selected = int(1/5 * len(y_true))\n",
    "    top_k_probas = np.argsort(y_pred)[-n_selected:]\n",
    "    true_positives = y_true[top_k_probas].sum()\n",
    "    \n",
    "    max_reward = n_selected * 10\n",
    "    reward = true_positives * 10 * 10000/max_reward\n",
    "    cost = data.num_col() * 200\n",
    "    # print(f\"reward: {reward}, cost: {cost}, max_reward: {max_reward}\")\n",
    "    return 'Euros_gained',- (reward - cost)\n",
    "\n",
    "class SaveBestModel(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, cvboosters):\n",
    "        self._cvboosters = cvboosters\n",
    "    \n",
    "    def after_training(self, model):\n",
    "        self._cvboosters[:] = [cvpack.bst for cvpack in model.cvfolds]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "629fd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)\n",
    "params = {'objective':'binary:logistic'\n",
    "          ,'eval_metric':'logloss',\n",
    "          'eta':0.01,\n",
    "          'max_depth' : 8,\n",
    "          'device' : 'cuda',\n",
    "          'colsample_bytree':0.85,\n",
    "          'verbosity' : 1,\n",
    "          'alpha' : 7,}\n",
    "\n",
    "\n",
    "cvboosters = []\n",
    "xgb_cv = xgb.cv(\n",
    "    dtrain=data_dmatrix,\n",
    "    params=params,\n",
    "    nfold=5,\n",
    "    metrics = 'logloss',\n",
    "    seed=42,\n",
    "    num_boost_round = 200,\n",
    "    custom_metric = xgb_euros_gained,\n",
    "    #early_stopping_rounds=20,\n",
    "    callbacks=[SaveBestModel(cvboosters), ],\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9112810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs, X_val_fs, y_train_fs, y_val_fs = model_selection.train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5384a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting feature no. 264 of importance score -0.0011764705882352678\n",
      "Deleting feature no. 265 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 296 of importance score -0.004470588235294138\n",
      "Deleting feature no. 75 of importance score -0.00352941176470587\n",
      "Deleting feature no. 1 of importance score -0.005647058823529361\n",
      "Deleting feature no. 79 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 2 of importance score -0.003529411764705914\n",
      "Deleting feature no. 211 of importance score -0.008235294117647073\n",
      "Deleting feature no. 315 of importance score -0.004235294117646981\n",
      "Deleting feature no. 54 of importance score -0.003294117647058803\n",
      "Deleting feature no. 219 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 255 of importance score -0.002823529411764669\n",
      "Deleting feature no. 68 of importance score -0.0035294117647059363\n",
      "Deleting feature no. 441 of importance score -0.003999999999999937\n",
      "Deleting feature no. 414 of importance score -0.002588235294117669\n",
      "Deleting feature no. 0 of importance score -0.002823529411764669\n",
      "Deleting feature no. 483 of importance score -0.003529411764705892\n",
      "Deleting feature no. 305 of importance score -0.004705882352941182\n",
      "Deleting feature no. 16 of importance score -0.0023529411764705356\n",
      "Deleting feature no. 344 of importance score -0.002823529411764758\n",
      "Deleting feature no. 212 of importance score -0.001882352941176424\n",
      "Deleting feature no. 208 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 350 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 461 of importance score -0.002117647058823513\n",
      "Deleting feature no. 55 of importance score -0.0032941176470588696\n",
      "Deleting feature no. 197 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 222 of importance score -0.002588235294117647\n",
      "Deleting feature no. 287 of importance score -0.003294117647058825\n",
      "Deleting feature no. 160 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 444 of importance score -0.001882352941176424\n",
      "Deleting feature no. 350 of importance score -0.003294117647058803\n",
      "Deleting feature no. 58 of importance score -0.0037647058823529144\n",
      "Deleting feature no. 364 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 451 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 35 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 196 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 1 of importance score -0.0037647058823529144\n",
      "Deleting feature no. 419 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 398 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 448 of importance score -0.001882352941176535\n",
      "Deleting feature no. 220 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 293 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 331 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 420 of importance score -0.001882352941176535\n",
      "Deleting feature no. 33 of importance score -0.0030588235294118247\n",
      "Deleting feature no. 340 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 318 of importance score -0.003529411764705914\n",
      "Deleting feature no. 358 of importance score -0.00235294117647058\n",
      "Deleting feature no. 344 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 200 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 74 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 284 of importance score -0.0016470588235293793\n",
      "Deleting feature no. 441 of importance score -0.001882352941176424\n",
      "Deleting feature no. 260 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 186 of importance score -0.00235294117647058\n",
      "Deleting feature no. 81 of importance score -0.004235294117647115\n",
      "Deleting feature no. 6 of importance score -0.002588235294117669\n",
      "Deleting feature no. 387 of importance score -0.0030588235294117805\n",
      "Deleting feature no. 287 of importance score -0.001411764705882379\n",
      "Deleting feature no. 101 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 48 of importance score -0.0030588235294118026\n",
      "Deleting feature no. 313 of importance score -0.002117647058823513\n",
      "Deleting feature no. 323 of importance score -0.00235294117647058\n",
      "Deleting feature no. 168 of importance score -0.002352941176470602\n",
      "Deleting feature no. 434 of importance score -0.0030588235294118026\n",
      "Deleting feature no. 427 of importance score -0.0037647058823529144\n",
      "Deleting feature no. 314 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 93 of importance score -0.003294117647058803\n",
      "Deleting feature no. 171 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 241 of importance score -0.0016470588235293793\n",
      "Deleting feature no. 166 of importance score -0.006352941176470584\n",
      "Deleting feature no. 33 of importance score -0.002588235294117691\n",
      "Deleting feature no. 72 of importance score -0.003294117647058803\n",
      "Deleting feature no. 202 of importance score -0.0014117647058824012\n",
      "Deleting feature no. 218 of importance score -0.001882352941176535\n",
      "Deleting feature no. 237 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 106 of importance score -0.002823529411764669\n",
      "Deleting feature no. 160 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 84 of importance score -0.002588235294117713\n",
      "Deleting feature no. 122 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 29 of importance score -0.001647058823529468\n",
      "Deleting feature no. 277 of importance score -0.0021176470588235795\n",
      "Deleting feature no. 32 of importance score -0.001882352941176535\n",
      "Deleting feature no. 339 of importance score -0.0035294117647058478\n",
      "Deleting feature no. 325 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 34 of importance score -0.002588235294117669\n",
      "Deleting feature no. 318 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 98 of importance score -0.0023529411764706245\n",
      "Deleting feature no. 125 of importance score -0.001882352941176535\n",
      "Deleting feature no. 336 of importance score -0.001647058823529468\n",
      "Deleting feature no. 330 of importance score -0.002117647058823513\n",
      "Deleting feature no. 274 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 301 of importance score -0.0030588235294117584\n",
      "Deleting feature no. 343 of importance score -0.003764705882352981\n",
      "Deleting feature no. 378 of importance score -0.002823529411764647\n",
      "Deleting feature no. 203 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 112 of importance score -0.002588235294117691\n",
      "Deleting feature no. 34 of importance score -0.003529411764705781\n",
      "Deleting feature no. 329 of importance score -0.004941176470588204\n",
      "Deleting feature no. 190 of importance score -0.0030588235294117584\n",
      "Deleting feature no. 137 of importance score -0.002352941176470602\n",
      "Deleting feature no. 10 of importance score -0.002352941176470602\n",
      "Deleting feature no. 371 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 25 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 187 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 58 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 141 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 254 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 67 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 196 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 293 of importance score -0.001647058823529468\n",
      "Deleting feature no. 366 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 261 of importance score -0.002823529411764647\n",
      "Deleting feature no. 182 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 361 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 83 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 291 of importance score -0.0018823529411765127\n",
      "Deleting feature no. 1 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 219 of importance score -0.0030588235294117137\n",
      "Deleting feature no. 259 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 368 of importance score -0.002823529411764736\n",
      "Deleting feature no. 116 of importance score -0.001882352941176424\n",
      "Deleting feature no. 191 of importance score -0.002588235294117713\n",
      "Deleting feature no. 275 of importance score -0.002588235294117669\n",
      "Deleting feature no. 220 of importance score -0.0023529411764706245\n",
      "Deleting feature no. 36 of importance score -0.0023529411764705134\n",
      "Deleting feature no. 176 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 171 of importance score -0.001882352941176535\n",
      "Deleting feature no. 92 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 278 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 341 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 91 of importance score -0.00235294117647058\n",
      "Deleting feature no. 355 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 361 of importance score -0.002588235294117713\n",
      "Deleting feature no. 302 of importance score -0.001647058823529468\n",
      "Deleting feature no. 282 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 38 of importance score -0.0021176470588234685\n",
      "Deleting feature no. 202 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 316 of importance score -0.001647058823529468\n",
      "Deleting feature no. 261 of importance score -0.001647058823529468\n",
      "Deleting feature no. 84 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 70 of importance score -0.0016470588235294014\n",
      "Deleting feature no. 27 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 291 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 317 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 85 of importance score -0.001882352941176535\n",
      "Deleting feature no. 328 of importance score -0.002823529411764736\n",
      "Deleting feature no. 62 of importance score -0.001882352941176535\n",
      "Deleting feature no. 274 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 167 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 103 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 233 of importance score -0.001882352941176535\n",
      "Deleting feature no. 146 of importance score -0.002352941176470602\n",
      "Deleting feature no. 148 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 303 of importance score -0.001882352941176424\n",
      "Deleting feature no. 276 of importance score -0.003764705882352981\n",
      "Deleting feature no. 209 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 318 of importance score -0.002588235294117647\n",
      "Deleting feature no. 259 of importance score -0.0011764705882352233\n",
      "Deleting feature no. 232 of importance score -0.0030588235294117137\n",
      "Deleting feature no. 213 of importance score -0.0011764705882353343\n",
      "Deleting feature no. 182 of importance score -0.001882352941176535\n",
      "Deleting feature no. 304 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 194 of importance score -0.001411764705882379\n",
      "Deleting feature no. 44 of importance score -0.002588235294117602\n",
      "Deleting feature no. 264 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 268 of importance score -0.003294117647058825\n",
      "Deleting feature no. 8 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 26 of importance score -0.0021176470588235574\n",
      "Deleting feature no. 123 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 14 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 5 of importance score -0.00258823529411758\n",
      "Deleting feature no. 189 of importance score -0.0030588235294117137\n",
      "Deleting feature no. 132 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 107 of importance score -0.002823529411764758\n",
      "Deleting feature no. 233 of importance score -0.002117647058823513\n",
      "Deleting feature no. 233 of importance score -0.002823529411764736\n",
      "Deleting feature no. 23 of importance score -0.0023529411764706687\n",
      "Deleting feature no. 241 of importance score -0.00258823529411758\n",
      "Deleting feature no. 132 of importance score -0.00235294117647058\n",
      "Deleting feature no. 80 of importance score -0.0021176470588235574\n",
      "Deleting feature no. 275 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 160 of importance score -0.004235294117647048\n",
      "Deleting feature no. 133 of importance score -0.002823529411764758\n",
      "Deleting feature no. 312 of importance score -0.0030588235294117584\n",
      "Deleting feature no. 299 of importance score -0.003294117647058825\n",
      "Deleting feature no. 185 of importance score -0.004235294117647026\n",
      "Deleting feature no. 125 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 134 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 231 of importance score -0.003294117647058803\n",
      "Deleting feature no. 221 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 166 of importance score -0.002823529411764736\n",
      "Deleting feature no. 257 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 143 of importance score -0.0032941176470588696\n",
      "Deleting feature no. 184 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 172 of importance score -0.002823529411764758\n",
      "Deleting feature no. 97 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 18 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 16 of importance score -0.0030588235294117584\n",
      "Deleting feature no. 228 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 268 of importance score -0.002352941176470602\n",
      "Deleting feature no. 118 of importance score -0.0023529411764706687\n",
      "Deleting feature no. 33 of importance score -0.002823529411764669\n",
      "Deleting feature no. 171 of importance score -0.002588235294117647\n",
      "Deleting feature no. 24 of importance score -0.001411764705882379\n",
      "Deleting feature no. 133 of importance score -0.002823529411764758\n",
      "Deleting feature no. 206 of importance score -0.002117647058823513\n",
      "Deleting feature no. 84 of importance score -0.002588235294117691\n",
      "Deleting feature no. 127 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 181 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 198 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 256 of importance score -0.002588235294117647\n",
      "Deleting feature no. 214 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 260 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 114 of importance score -0.002352941176470602\n",
      "Deleting feature no. 149 of importance score -0.003294117647058825\n",
      "Deleting feature no. 17 of importance score -0.0011764705882353343\n",
      "Deleting feature no. 190 of importance score -0.002588235294117713\n",
      "Deleting feature no. 16 of importance score -0.001882352941176424\n",
      "Deleting feature no. 102 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 98 of importance score -0.0030588235294118473\n",
      "Deleting feature no. 227 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 163 of importance score -0.0021176470588235795\n",
      "Deleting feature no. 229 of importance score -0.002588235294117669\n",
      "Deleting feature no. 194 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 263 of importance score -0.0018823529411764017\n",
      "Deleting feature no. 266 of importance score -0.0014117647058824012\n",
      "Deleting feature no. 103 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 162 of importance score -0.003529411764705914\n",
      "Deleting feature no. 103 of importance score -0.0021176470588235574\n",
      "Deleting feature no. 73 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 23 of importance score -0.0016470588235293793\n",
      "Deleting feature no. 162 of importance score -0.0021176470588234685\n",
      "Deleting feature no. 31 of importance score -0.001882352941176424\n",
      "Deleting feature no. 218 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 224 of importance score -0.0030588235294118247\n",
      "Deleting feature no. 215 of importance score -0.004235294117646981\n",
      "Deleting feature no. 137 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 199 of importance score -0.002823529411764758\n",
      "Deleting feature no. 78 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 3 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 14 of importance score -0.0030588235294116916\n",
      "Deleting feature no. 34 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 86 of importance score -0.002823529411764647\n",
      "Deleting feature no. 208 of importance score -0.0011764705882352678\n",
      "Deleting feature no. 235 of importance score -0.0014117647058824012\n",
      "Deleting feature no. 143 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 200 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 191 of importance score -0.0030588235294117137\n",
      "Deleting feature no. 18 of importance score -0.001411764705882379\n",
      "Deleting feature no. 11 of importance score -0.001882352941176424\n",
      "Deleting feature no. 150 of importance score -0.0030588235294117805\n",
      "Deleting feature no. 171 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 83 of importance score -0.002823529411764647\n",
      "Deleting feature no. 74 of importance score -0.003294117647058803\n",
      "Deleting feature no. 53 of importance score -0.0030588235294117805\n",
      "Deleting feature no. 34 of importance score -0.00235294117647058\n",
      "Deleting feature no. 135 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 171 of importance score -0.0030588235294118026\n",
      "Deleting feature no. 127 of importance score -0.0030588235294116916\n",
      "Deleting feature no. 18 of importance score -0.002823529411764736\n",
      "Deleting feature no. 204 of importance score -0.00258823529411758\n",
      "Deleting feature no. 195 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 161 of importance score -0.002588235294117647\n",
      "Deleting feature no. 177 of importance score -0.001647058823529468\n",
      "Deleting feature no. 229 of importance score -0.002588235294117691\n",
      "Deleting feature no. 106 of importance score -0.002117647058823513\n",
      "Deleting feature no. 228 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 153 of importance score -0.002823529411764669\n",
      "Deleting feature no. 32 of importance score -0.001882352941176424\n",
      "Deleting feature no. 220 of importance score -0.002117647058823513\n",
      "Deleting feature no. 121 of importance score -0.0016470588235293793\n",
      "Deleting feature no. 61 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 63 of importance score -0.002588235294117647\n",
      "Deleting feature no. 134 of importance score -0.001411764705882379\n",
      "Deleting feature no. 104 of importance score -0.001882352941176424\n",
      "Deleting feature no. 5 of importance score -0.002117647058823513\n",
      "Deleting feature no. 146 of importance score -0.001647058823529357\n",
      "Deleting feature no. 146 of importance score -0.001882352941176424\n",
      "Deleting feature no. 117 of importance score -0.001647058823529468\n",
      "Deleting feature no. 163 of importance score -0.001411764705882379\n",
      "Deleting feature no. 24 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 154 of importance score -0.0021176470588235574\n",
      "Deleting feature no. 37 of importance score -0.002823529411764758\n",
      "Deleting feature no. 72 of importance score -0.002823529411764758\n",
      "Deleting feature no. 49 of importance score -0.0023529411764706245\n",
      "Deleting feature no. 127 of importance score -0.0016470588235294459\n",
      "Deleting feature no. 40 of importance score -0.0011764705882353343\n",
      "Deleting feature no. 133 of importance score -0.0021176470588235795\n",
      "Deleting feature no. 66 of importance score -0.002823529411764647\n",
      "Deleting feature no. 50 of importance score -0.002588235294117602\n",
      "Deleting feature no. 205 of importance score -0.0011764705882352454\n",
      "Deleting feature no. 13 of importance score -0.002823529411764758\n",
      "Deleting feature no. 126 of importance score -0.004000000000000026\n",
      "Deleting feature no. 202 of importance score -0.00235294117647058\n",
      "Deleting feature no. 9 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 4 of importance score -0.0023529411764706245\n",
      "Deleting feature no. 86 of importance score -0.002117647058823513\n",
      "Deleting feature no. 55 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 3 of importance score -0.001882352941176424\n",
      "Deleting feature no. 103 of importance score -0.003294117647058825\n",
      "Deleting feature no. 158 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 187 of importance score -0.002117647058823513\n",
      "Deleting feature no. 153 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 136 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 104 of importance score -0.004235294117647026\n",
      "Deleting feature no. 151 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 112 of importance score -0.002823529411764625\n",
      "Deleting feature no. 149 of importance score -0.002117647058823513\n",
      "Deleting feature no. 134 of importance score -0.002588235294117713\n",
      "Deleting feature no. 143 of importance score -0.0035294117647058478\n",
      "Deleting feature no. 124 of importance score -0.0032941176470588475\n",
      "Deleting feature no. 106 of importance score -0.0016470588235294014\n",
      "Deleting feature no. 74 of importance score -0.004470588235294138\n",
      "Deleting feature no. 14 of importance score -0.0011764705882353343\n",
      "Deleting feature no. 8 of importance score -0.0023529411764706245\n",
      "Deleting feature no. 182 of importance score -0.0023529411764706687\n",
      "Deleting feature no. 172 of importance score -0.001882352941176424\n",
      "Deleting feature no. 58 of importance score -0.0023529411764706687\n",
      "Deleting feature no. 155 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 106 of importance score -0.002823529411764669\n",
      "Deleting feature no. 18 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 72 of importance score -0.0018823529411764683\n",
      "Deleting feature no. 120 of importance score -0.002588235294117647\n",
      "Deleting feature no. 168 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 117 of importance score -0.0023529411764705577\n",
      "Deleting feature no. 143 of importance score -0.00235294117647058\n",
      "Deleting feature no. 171 of importance score -0.0014117647058822902\n",
      "Deleting feature no. 60 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 124 of importance score -0.002588235294117647\n",
      "Deleting feature no. 46 of importance score -0.0016470588235293793\n",
      "Deleting feature no. 90 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 52 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 13 of importance score -0.0018823529411764906\n",
      "Deleting feature no. 142 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 34 of importance score -0.0030588235294116916\n",
      "Deleting feature no. 12 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 101 of importance score -0.001882352941176424\n",
      "Deleting feature no. 44 of importance score -0.003294117647058825\n",
      "Deleting feature no. 86 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 75 of importance score -0.002352941176470602\n",
      "Deleting feature no. 113 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 11 of importance score -0.0021176470588235574\n",
      "Deleting feature no. 35 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 128 of importance score -0.00235294117647058\n",
      "Deleting feature no. 13 of importance score -0.001882352941176535\n",
      "Deleting feature no. 141 of importance score -0.0014117647058822902\n",
      "Deleting feature no. 60 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 3 of importance score -0.0009411764705882675\n",
      "Deleting feature no. 13 of importance score -0.0011764705882353343\n",
      "Deleting feature no. 52 of importance score -0.0032941176470587586\n",
      "Deleting feature no. 66 of importance score -0.002823529411764669\n",
      "Deleting feature no. 18 of importance score -0.002117647058823513\n",
      "Deleting feature no. 101 of importance score -0.0030588235294118026\n",
      "Deleting feature no. 127 of importance score -0.002588235294117647\n",
      "Deleting feature no. 47 of importance score -0.0018823529411765127\n",
      "Deleting feature no. 42 of importance score -0.001647058823529468\n",
      "Deleting feature no. 115 of importance score -0.00352941176470587\n",
      "Deleting feature no. 104 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 119 of importance score -0.002823529411764625\n",
      "Deleting feature no. 128 of importance score -0.002823529411764736\n",
      "Deleting feature no. 65 of importance score -0.002588235294117647\n",
      "Deleting feature no. 51 of importance score -0.0014117647058823567\n",
      "Deleting feature no. 122 of importance score -0.004705882352941138\n",
      "Deleting feature no. 2 of importance score -0.0028235294117646913\n",
      "Deleting feature no. 31 of importance score -0.0037647058823529144\n",
      "Deleting feature no. 87 of importance score -0.0009411764705882008\n",
      "Deleting feature no. 45 of importance score -0.005647058823529472\n",
      "Deleting feature no. 89 of importance score -0.0009411764705882675\n",
      "Deleting feature no. 63 of importance score -0.003529411764705803\n",
      "Deleting feature no. 31 of importance score -0.002588235294117647\n",
      "Deleting feature no. 16 of importance score -0.002588235294117669\n",
      "Deleting feature no. 21 of importance score -0.0016470588235293793\n",
      "Deleting feature no. 121 of importance score -0.0016470588235294459\n",
      "Deleting feature no. 15 of importance score -0.0018823529411764017\n",
      "Deleting feature no. 12 of importance score -0.00235294117647058\n",
      "Deleting feature no. 23 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 79 of importance score -0.0030588235294117805\n",
      "Deleting feature no. 121 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 109 of importance score -0.001882352941176424\n",
      "Deleting feature no. 112 of importance score -0.001882352941176424\n",
      "Deleting feature no. 113 of importance score -0.0014117647058823567\n",
      "Deleting feature no. 14 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 62 of importance score -0.0021176470588235795\n",
      "Deleting feature no. 69 of importance score -0.0030588235294117584\n",
      "Deleting feature no. 31 of importance score -0.001882352941176424\n",
      "Deleting feature no. 38 of importance score -0.002588235294117647\n",
      "Deleting feature no. 105 of importance score -0.0014117647058823567\n",
      "Deleting feature no. 42 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 37 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 82 of importance score -0.0014117647058823346\n",
      "Deleting feature no. 5 of importance score -0.002588235294117669\n",
      "Deleting feature no. 61 of importance score -0.002588235294117669\n",
      "Deleting feature no. 12 of importance score -0.00023529411764704465\n",
      "Deleting feature no. 49 of importance score -0.0014117647058823567\n",
      "Deleting feature no. 25 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 61 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 98 of importance score -0.0016470588235294238\n",
      "Deleting feature no. 33 of importance score -0.00047058823529415593\n",
      "Deleting feature no. 76 of importance score -0.003529411764705914\n",
      "Deleting feature no. 37 of importance score -0.0016470588235294014\n",
      "Deleting feature no. 70 of importance score -0.0030588235294117584\n",
      "Deleting feature no. 38 of importance score -0.0037647058823530033\n",
      "Deleting feature no. 87 of importance score -0.001882352941176424\n",
      "Deleting feature no. 82 of importance score -0.002823529411764758\n",
      "Deleting feature no. 20 of importance score -0.003294117647058825\n",
      "Deleting feature no. 46 of importance score -0.004705882352941226\n",
      "Deleting feature no. 25 of importance score -0.0051764705882353605\n",
      "Deleting feature no. 2 of importance score -0.0030588235294118026\n",
      "Deleting feature no. 38 of importance score -0.004470588235294138\n",
      "Deleting feature no. 58 of importance score -0.004000000000000026\n",
      "Deleting feature no. 14 of importance score -0.001882352941176424\n",
      "Deleting feature no. 87 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 85 of importance score -0.003294117647058825\n",
      "Deleting feature no. 43 of importance score -0.00235294117647058\n",
      "Deleting feature no. 58 of importance score -0.0023529411764706466\n",
      "Deleting feature no. 63 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 8 of importance score -0.0030588235294117362\n",
      "Deleting feature no. 39 of importance score -0.002588235294117647\n",
      "Deleting feature no. 11 of importance score -0.001647058823529468\n",
      "Deleting feature no. 45 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 12 of importance score -0.003529411764705914\n",
      "Deleting feature no. 7 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 8 of importance score -0.0021176470588235795\n",
      "Deleting feature no. 29 of importance score -0.002823529411764758\n",
      "Deleting feature no. 55 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 30 of importance score -0.0004705882352940893\n",
      "Deleting feature no. 9 of importance score -0.003294117647058825\n",
      "Deleting feature no. 66 of importance score -0.0030588235294117805\n",
      "Deleting feature no. 50 of importance score -0.0016470588235294014\n",
      "Deleting feature no. 55 of importance score -0.00235294117647058\n",
      "Deleting feature no. 44 of importance score -0.002588235294117691\n",
      "Deleting feature no. 26 of importance score -0.001882352941176535\n",
      "Deleting feature no. 17 of importance score -0.0016470588235294459\n",
      "Deleting feature no. 16 of importance score -0.0009411764705882231\n",
      "Deleting feature no. 47 of importance score -0.002588235294117647\n",
      "Deleting feature no. 25 of importance score -0.0032941176470588475\n",
      "Deleting feature no. 12 of importance score -0.003529411764705892\n",
      "Deleting feature no. 11 of importance score -0.0018823529411764461\n",
      "Deleting feature no. 6 of importance score -0.004470588235294115\n",
      "Deleting feature no. 33 of importance score -0.002352941176470602\n",
      "Deleting feature no. 34 of importance score -0.0035294117647059363\n",
      "Deleting feature no. 56 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 25 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 53 of importance score -0.0021176470588234906\n",
      "Deleting feature no. 30 of importance score -0.0016470588235294459\n",
      "Deleting feature no. 13 of importance score -0.0011764705882352678\n",
      "Deleting feature no. 7 of importance score -0.0014117647058823125\n",
      "Deleting feature no. 11 of importance score -0.0021176470588236017\n",
      "Deleting feature no. 18 of importance score -0.0011764705882352454\n",
      "Deleting feature no. 2 of importance score -0.002823529411764736\n",
      "Deleting feature no. 18 of importance score -0.001647058823529357\n",
      "Deleting feature no. 12 of importance score -0.0009411764705882453\n",
      "Deleting feature no. 8 of importance score -0.0028235294117647802\n",
      "Deleting feature no. 29 of importance score -0.002117647058823513\n",
      "Deleting feature no. 19 of importance score -0.0014117647058824012\n",
      "Deleting feature no. 43 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 19 of importance score -0.0014117647058824012\n",
      "Deleting feature no. 24 of importance score -0.0021176470588235795\n",
      "Deleting feature no. 9 of importance score -0.0011764705882353122\n",
      "Deleting feature no. 23 of importance score -0.0032941176470588918\n",
      "Deleting feature no. 18 of importance score -0.0056470588235294494\n",
      "Deleting feature no. 30 of importance score -0.0007058823529411118\n",
      "Deleting feature no. 14 of importance score -0.002823529411764669\n",
      "Deleting feature no. 25 of importance score -0.0032941176470588475\n",
      "Deleting feature no. 12 of importance score -0.00023529411764702246\n",
      "Deleting feature no. 11 of importance score -0.0028235294117647134\n",
      "Deleting feature no. 22 of importance score -0.0009411764705882453\n",
      "Deleting feature no. 28 of importance score -0.0030588235294117137\n",
      "Deleting feature no. 10 of importance score -0.002117647058823513\n",
      "Deleting feature no. 7 of importance score -0.0021176470588235574\n",
      "Deleting feature no. 27 of importance score -0.004705882352941204\n",
      "Deleting feature no. 20 of importance score -0.0016470588235294459\n",
      "Deleting feature no. 24 of importance score 0.00047058823529406714\n",
      "Deleting feature no. 22 of importance score -0.00023529411764706688\n",
      "Deleting feature no. 8 of importance score -0.0007058823529411562\n",
      "Deleting feature no. 6 of importance score -0.0009411764705882453\n",
      "Deleting feature no. 8 of importance score 0.0014117647058823567\n",
      "Deleting feature no. 3 of importance score -0.0021176470588235353\n",
      "Deleting feature no. 13 of importance score -0.0007058823529411562\n",
      "Deleting feature no. 11 of importance score -0.0011764705882353122\n",
      "Deleting feature no. 3 of importance score -0.0025882352941176247\n",
      "Deleting feature no. 3 of importance score 0.0009411764705881786\n",
      "Deleting feature no. 9 of importance score -0.0009411764705882453\n",
      "Deleting feature no. 14 of importance score -0.0040000000000000036\n",
      "Deleting feature no. 10 of importance score 0.0014117647058823567\n",
      "Deleting feature no. 6 of importance score 0.003529411764705892\n",
      "Deleting feature no. 6 of importance score 0.0011764705882352678\n",
      "Deleting feature no. 4 of importance score 0.0018823529411764461\n",
      "Deleting feature no. 4 of importance score 0.0028235294117647134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "X_train_fs_iter = X_train_fs.copy(deep=True)\n",
    "X_val_fs_iter =X_val_fs.copy(deep=True)\n",
    "feature_args = np.array([i for i in range(500)])\n",
    "deleted_features_arg = []\n",
    "for i in range(500):\n",
    "    clf = xgb.XGBClassifier(eta=0.01, device = 'cuda', n_estimators=150, max_depth=10)\n",
    "    clf.fit(X_train_fs_iter, y_train_fs)\n",
    "    r = permutation_importance(clf, X_val_fs_iter, y_val_fs,n_repeats=5,random_state=0)\n",
    "    min_importance_arg = r.importances_mean.argsort()[0]\n",
    "    min_importance = r.importances_mean[min_importance_arg]\n",
    "    if len(feature_args) > 10:\n",
    "        print(f\"Deleting feature no. {min_importance_arg} of importance score {min_importance}\")\n",
    "        feature_args = np.delete(feature_args, min_importance_arg)\n",
    "        X_train_fs_iter = X_train_fs.copy(deep=True).iloc[:, feature_args]\n",
    "        X_val_fs_iter = X_val_fs.copy(deep=True).iloc[:, feature_args]\n",
    "    else:\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4469bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_feature_args_from_str(log_text : str, drop_limit = 498):\n",
    "    f_args = list(range(500))\n",
    "    deleted_features = list(map(int, re.findall(r\"Deleting feature no\\. (\\d+)\", log_text)))\n",
    "    for i, d in enumerate(deleted_features):\n",
    "        f_args = np.delete(f_args, d)\n",
    "\n",
    "        if i >= drop_limit:\n",
    "            break\n",
    "\n",
    "    return f_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54fc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting feature no. 1 of importance score 0.08211764705882355\n",
      "Deleting feature no. 0 of importance score 0.08094117647058828\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[275]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m500\u001b[39m):\n\u001b[32m      8\u001b[39m     clf = xgb.XGBClassifier(eta=\u001b[32m0.01\u001b[39m, device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, n_estimators=\u001b[32m150\u001b[39m, max_depth=\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fs_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     r = permutation_importance(clf, X_val_fs_iter, y_val_fs,n_repeats=\u001b[32m5\u001b[39m,random_state=\u001b[32m0\u001b[39m)\n\u001b[32m     11\u001b[39m     min_importance_arg = r.importances_mean.argsort()[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/sklearn.py:1663\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1658\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mnum_class\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.n_classes_\n\u001b[32m   1660\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1661\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1662\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m train_dmatrix, evals = \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = train(\n\u001b[32m   1683\u001b[39m     params,\n\u001b[32m   1684\u001b[39m     train_dmatrix,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1693\u001b[39m     callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   1694\u001b[39m )\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/sklearn.py:628\u001b[39m, in \u001b[36m_wrap_evaluation_matrices\u001b[39m\u001b[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_evaluation_matrices\u001b[39m(\n\u001b[32m    608\u001b[39m     *,\n\u001b[32m    609\u001b[39m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m     feature_types: Optional[FeatureTypes],\n\u001b[32m    625\u001b[39m ) -> Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m    626\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[33;03m    way.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     train_dmatrix = \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m     n_validation = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) -> Sequence:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/sklearn.py:1137\u001b[39m, in \u001b[36mXGBModel._create_dmatrix\u001b[39m\u001b[34m(self, ref, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m.tree_method, \u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.booster != \u001b[33m\"\u001b[39m\u001b[33mgblinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bin\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:1614\u001b[39m, in \u001b[36mQuantileDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1595\u001b[39m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1607\u001b[39m         )\n\u001b[32m   1608\u001b[39m     ):\n\u001b[32m   1609\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf data iterator is used as input, data like label should be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecified as batch argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1612\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:1678\u001b[39m, in \u001b[36mQuantileDMatrix._init\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1663\u001b[39m config = make_jcargs(\n\u001b[32m   1664\u001b[39m     nthread=\u001b[38;5;28mself\u001b[39m.nthread,\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     max_bin=\u001b[38;5;28mself\u001b[39m.max_bin,\n\u001b[32m   1667\u001b[39m     max_quantile_blocks=max_quantile_blocks,\n\u001b[32m   1668\u001b[39m )\n\u001b[32m   1669\u001b[39m ret = _LIB.XGQuantileDMatrixCreateFromCallback(\n\u001b[32m   1670\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1671\u001b[39m     it.proxy.handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1676\u001b[39m     ctypes.byref(handle),\n\u001b[32m   1677\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m \u001b[43mit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m   1680\u001b[39m _check_call(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:572\u001b[39m, in \u001b[36mDataIter.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    570\u001b[39m exc = \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:553\u001b[39m, in \u001b[36mDataIter._handle_exception\u001b[39m\u001b[34m(self, fn, dft_ret)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[32m    558\u001b[39m     tb = sys.exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:640\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m._temporary_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m), \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/data.py:1654\u001b[39m, in \u001b[36mSingleBatchInternalIter.next\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m   1652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28mself\u001b[39m.it += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/core.py:628\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.input_data\u001b[39m\u001b[34m(data, feature_names, feature_types, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28mself\u001b[39m._temporary_data = (new, cat_codes, feature_names, feature_types)\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[43mdispatch_proxy_set_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_codes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m.proxy.set_info(\n\u001b[32m    630\u001b[39m     feature_names=feature_names,\n\u001b[32m    631\u001b[39m     feature_types=feature_types,\n\u001b[32m    632\u001b[39m     **kwargs,\n\u001b[32m    633\u001b[39m )\n\u001b[32m    634\u001b[39m \u001b[38;5;28mself\u001b[39m._data_ref = ref\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/data.py:1730\u001b[39m, in \u001b[36mdispatch_proxy_set_data\u001b[39m\u001b[34m(proxy, data, cat_codes)\u001b[39m\n\u001b[32m   1724\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Dispatch for QuantileDMatrix.\"\"\"\u001b[39;00m\n\u001b[32m   1725\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1726\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m _is_cudf_ser(data)\n\u001b[32m   1727\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pandas_series(data)\n\u001b[32m   1728\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_polars_series(data)\n\u001b[32m   1729\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     \u001b[43m_check_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data):\n\u001b[32m   1733\u001b[39m     \u001b[38;5;66;03m# pylint: disable=W0212\u001b[39;00m\n\u001b[32m   1734\u001b[39m     proxy._ref_data_from_cuda_columnar(data, cast(List, cat_codes))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/data.py:85\u001b[39m, in \u001b[36m_check_data_shape\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_data_shape\u001b[39m(data: DataType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.shape) != \u001b[32m2\u001b[39m:\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease reshape the input data into 2-dimensional matrix.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Envs/AML/lib64/python3.13/site-packages/xgboost/data.py:627\u001b[39m, in \u001b[36mPandasTransformed.shape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    626\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return shape of the transformed DataFrame.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "feature_args = np.array(selected_features)\n",
    "X_train_fs_iter = X_train_fs.copy(deep=True).iloc[:, feature_args]\n",
    "X_val_fs_iter =X_val_fs.copy(deep=True).iloc[:, feature_args]\n",
    "deleted_features_arg = []\n",
    "for i in range(500):\n",
    "    clf = xgb.XGBClassifier(eta=0.01, device = 'cuda', n_estimators=150, max_depth=10)\n",
    "    clf.fit(X_train_fs_iter, y_train_fs)\n",
    "    r = permutation_importance(clf, X_val_fs_iter, y_val_fs,n_repeats=5,random_state=0)\n",
    "    min_importance_arg = r.importances_mean.argsort()[0]\n",
    "    min_importance = r.importances_mean[min_importance_arg]\n",
    "    if len(feature_args) > 2:\n",
    "        print(f\"Deleting feature no. {min_importance_arg} of importance score {min_importance}\")\n",
    "        feature_args = np.delete(feature_args, min_importance_arg)\n",
    "        X_train_fs_iter = X_train_fs.copy(deep=True).iloc[:, feature_args]\n",
    "        X_val_fs_iter = X_val_fs.copy(deep=True).iloc[:, feature_args]\n",
    "    else:\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864e9a0",
   "metadata": {},
   "source": [
    "# Model on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6edac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective':'binary:logistic',\n",
    "          'eval_metric':'logloss',\n",
    "          'eta':0.1,\n",
    "          'n_estimators' : 400,\n",
    "          'max_depth' : 8,\n",
    "          'device' : 'cuda',\n",
    "          'colsample_bytree':0.8,\n",
    "          'verbosity' : 1,\n",
    "          'alpha' : 7,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "d627832a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   6, 414, 425, 462])"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_text = \"\"\"\n",
    "Deleting feature no. 264 of importance score -0.0011764705882352678\n",
    "Deleting feature no. 265 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 296 of importance score -0.004470588235294138\n",
    "Deleting feature no. 75 of importance score -0.00352941176470587\n",
    "Deleting feature no. 1 of importance score -0.005647058823529361\n",
    "Deleting feature no. 79 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 2 of importance score -0.003529411764705914\n",
    "Deleting feature no. 211 of importance score -0.008235294117647073\n",
    "Deleting feature no. 315 of importance score -0.004235294117646981\n",
    "Deleting feature no. 54 of importance score -0.003294117647058803\n",
    "Deleting feature no. 219 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 255 of importance score -0.002823529411764669\n",
    "Deleting feature no. 68 of importance score -0.0035294117647059363\n",
    "Deleting feature no. 441 of importance score -0.003999999999999937\n",
    "Deleting feature no. 414 of importance score -0.002588235294117669\n",
    "Deleting feature no. 0 of importance score -0.002823529411764669\n",
    "Deleting feature no. 483 of importance score -0.003529411764705892\n",
    "Deleting feature no. 305 of importance score -0.004705882352941182\n",
    "Deleting feature no. 16 of importance score -0.0023529411764705356\n",
    "Deleting feature no. 344 of importance score -0.002823529411764758\n",
    "Deleting feature no. 212 of importance score -0.001882352941176424\n",
    "Deleting feature no. 208 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 350 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 461 of importance score -0.002117647058823513\n",
    "Deleting feature no. 55 of importance score -0.0032941176470588696\n",
    "Deleting feature no. 197 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 222 of importance score -0.002588235294117647\n",
    "Deleting feature no. 287 of importance score -0.003294117647058825\n",
    "Deleting feature no. 160 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 444 of importance score -0.001882352941176424\n",
    "Deleting feature no. 350 of importance score -0.003294117647058803\n",
    "Deleting feature no. 58 of importance score -0.0037647058823529144\n",
    "Deleting feature no. 364 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 451 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 35 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 196 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 1 of importance score -0.0037647058823529144\n",
    "Deleting feature no. 419 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 398 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 448 of importance score -0.001882352941176535\n",
    "Deleting feature no. 220 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 293 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 331 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 420 of importance score -0.001882352941176535\n",
    "Deleting feature no. 33 of importance score -0.0030588235294118247\n",
    "Deleting feature no. 340 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 318 of importance score -0.003529411764705914\n",
    "Deleting feature no. 358 of importance score -0.00235294117647058\n",
    "Deleting feature no. 344 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 200 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 74 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 284 of importance score -0.0016470588235293793\n",
    "Deleting feature no. 441 of importance score -0.001882352941176424\n",
    "Deleting feature no. 260 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 186 of importance score -0.00235294117647058\n",
    "Deleting feature no. 81 of importance score -0.004235294117647115\n",
    "Deleting feature no. 6 of importance score -0.002588235294117669\n",
    "Deleting feature no. 387 of importance score -0.0030588235294117805\n",
    "Deleting feature no. 287 of importance score -0.001411764705882379\n",
    "Deleting feature no. 101 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 48 of importance score -0.0030588235294118026\n",
    "Deleting feature no. 313 of importance score -0.002117647058823513\n",
    "Deleting feature no. 323 of importance score -0.00235294117647058\n",
    "Deleting feature no. 168 of importance score -0.002352941176470602\n",
    "Deleting feature no. 434 of importance score -0.0030588235294118026\n",
    "Deleting feature no. 427 of importance score -0.0037647058823529144\n",
    "Deleting feature no. 314 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 93 of importance score -0.003294117647058803\n",
    "Deleting feature no. 171 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 241 of importance score -0.0016470588235293793\n",
    "Deleting feature no. 166 of importance score -0.006352941176470584\n",
    "Deleting feature no. 33 of importance score -0.002588235294117691\n",
    "Deleting feature no. 72 of importance score -0.003294117647058803\n",
    "Deleting feature no. 202 of importance score -0.0014117647058824012\n",
    "Deleting feature no. 218 of importance score -0.001882352941176535\n",
    "Deleting feature no. 237 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 106 of importance score -0.002823529411764669\n",
    "Deleting feature no. 160 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 84 of importance score -0.002588235294117713\n",
    "Deleting feature no. 122 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 29 of importance score -0.001647058823529468\n",
    "Deleting feature no. 277 of importance score -0.0021176470588235795\n",
    "Deleting feature no. 32 of importance score -0.001882352941176535\n",
    "Deleting feature no. 339 of importance score -0.0035294117647058478\n",
    "Deleting feature no. 325 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 34 of importance score -0.002588235294117669\n",
    "Deleting feature no. 318 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 98 of importance score -0.0023529411764706245\n",
    "Deleting feature no. 125 of importance score -0.001882352941176535\n",
    "Deleting feature no. 336 of importance score -0.001647058823529468\n",
    "Deleting feature no. 330 of importance score -0.002117647058823513\n",
    "Deleting feature no. 274 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 301 of importance score -0.0030588235294117584\n",
    "Deleting feature no. 343 of importance score -0.003764705882352981\n",
    "Deleting feature no. 378 of importance score -0.002823529411764647\n",
    "Deleting feature no. 203 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 112 of importance score -0.002588235294117691\n",
    "Deleting feature no. 34 of importance score -0.003529411764705781\n",
    "Deleting feature no. 329 of importance score -0.004941176470588204\n",
    "Deleting feature no. 190 of importance score -0.0030588235294117584\n",
    "Deleting feature no. 137 of importance score -0.002352941176470602\n",
    "Deleting feature no. 10 of importance score -0.002352941176470602\n",
    "Deleting feature no. 371 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 25 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 187 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 58 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 141 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 254 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 67 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 196 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 293 of importance score -0.001647058823529468\n",
    "Deleting feature no. 366 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 261 of importance score -0.002823529411764647\n",
    "Deleting feature no. 182 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 361 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 83 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 291 of importance score -0.0018823529411765127\n",
    "Deleting feature no. 1 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 219 of importance score -0.0030588235294117137\n",
    "Deleting feature no. 259 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 368 of importance score -0.002823529411764736\n",
    "Deleting feature no. 116 of importance score -0.001882352941176424\n",
    "Deleting feature no. 191 of importance score -0.002588235294117713\n",
    "Deleting feature no. 275 of importance score -0.002588235294117669\n",
    "Deleting feature no. 220 of importance score -0.0023529411764706245\n",
    "Deleting feature no. 36 of importance score -0.0023529411764705134\n",
    "Deleting feature no. 176 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 171 of importance score -0.001882352941176535\n",
    "Deleting feature no. 92 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 278 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 341 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 91 of importance score -0.00235294117647058\n",
    "Deleting feature no. 355 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 361 of importance score -0.002588235294117713\n",
    "Deleting feature no. 302 of importance score -0.001647058823529468\n",
    "Deleting feature no. 282 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 38 of importance score -0.0021176470588234685\n",
    "Deleting feature no. 202 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 316 of importance score -0.001647058823529468\n",
    "Deleting feature no. 261 of importance score -0.001647058823529468\n",
    "Deleting feature no. 84 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 70 of importance score -0.0016470588235294014\n",
    "Deleting feature no. 27 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 291 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 317 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 85 of importance score -0.001882352941176535\n",
    "Deleting feature no. 328 of importance score -0.002823529411764736\n",
    "Deleting feature no. 62 of importance score -0.001882352941176535\n",
    "Deleting feature no. 274 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 167 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 103 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 233 of importance score -0.001882352941176535\n",
    "Deleting feature no. 146 of importance score -0.002352941176470602\n",
    "Deleting feature no. 148 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 303 of importance score -0.001882352941176424\n",
    "Deleting feature no. 276 of importance score -0.003764705882352981\n",
    "Deleting feature no. 209 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 318 of importance score -0.002588235294117647\n",
    "Deleting feature no. 259 of importance score -0.0011764705882352233\n",
    "Deleting feature no. 232 of importance score -0.0030588235294117137\n",
    "Deleting feature no. 213 of importance score -0.0011764705882353343\n",
    "Deleting feature no. 182 of importance score -0.001882352941176535\n",
    "Deleting feature no. 304 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 194 of importance score -0.001411764705882379\n",
    "Deleting feature no. 44 of importance score -0.002588235294117602\n",
    "Deleting feature no. 264 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 268 of importance score -0.003294117647058825\n",
    "Deleting feature no. 8 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 26 of importance score -0.0021176470588235574\n",
    "Deleting feature no. 123 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 14 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 5 of importance score -0.00258823529411758\n",
    "Deleting feature no. 189 of importance score -0.0030588235294117137\n",
    "Deleting feature no. 132 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 107 of importance score -0.002823529411764758\n",
    "Deleting feature no. 233 of importance score -0.002117647058823513\n",
    "Deleting feature no. 233 of importance score -0.002823529411764736\n",
    "Deleting feature no. 23 of importance score -0.0023529411764706687\n",
    "Deleting feature no. 241 of importance score -0.00258823529411758\n",
    "Deleting feature no. 132 of importance score -0.00235294117647058\n",
    "Deleting feature no. 80 of importance score -0.0021176470588235574\n",
    "Deleting feature no. 275 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 160 of importance score -0.004235294117647048\n",
    "Deleting feature no. 133 of importance score -0.002823529411764758\n",
    "Deleting feature no. 312 of importance score -0.0030588235294117584\n",
    "Deleting feature no. 299 of importance score -0.003294117647058825\n",
    "Deleting feature no. 185 of importance score -0.004235294117647026\n",
    "Deleting feature no. 125 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 134 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 231 of importance score -0.003294117647058803\n",
    "Deleting feature no. 221 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 166 of importance score -0.002823529411764736\n",
    "Deleting feature no. 257 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 143 of importance score -0.0032941176470588696\n",
    "Deleting feature no. 184 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 172 of importance score -0.002823529411764758\n",
    "Deleting feature no. 97 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 18 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 16 of importance score -0.0030588235294117584\n",
    "Deleting feature no. 228 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 268 of importance score -0.002352941176470602\n",
    "Deleting feature no. 118 of importance score -0.0023529411764706687\n",
    "Deleting feature no. 33 of importance score -0.002823529411764669\n",
    "Deleting feature no. 171 of importance score -0.002588235294117647\n",
    "Deleting feature no. 24 of importance score -0.001411764705882379\n",
    "Deleting feature no. 133 of importance score -0.002823529411764758\n",
    "Deleting feature no. 206 of importance score -0.002117647058823513\n",
    "Deleting feature no. 84 of importance score -0.002588235294117691\n",
    "Deleting feature no. 127 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 181 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 198 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 256 of importance score -0.002588235294117647\n",
    "Deleting feature no. 214 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 260 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 114 of importance score -0.002352941176470602\n",
    "Deleting feature no. 149 of importance score -0.003294117647058825\n",
    "Deleting feature no. 17 of importance score -0.0011764705882353343\n",
    "Deleting feature no. 190 of importance score -0.002588235294117713\n",
    "Deleting feature no. 16 of importance score -0.001882352941176424\n",
    "Deleting feature no. 102 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 98 of importance score -0.0030588235294118473\n",
    "Deleting feature no. 227 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 163 of importance score -0.0021176470588235795\n",
    "Deleting feature no. 229 of importance score -0.002588235294117669\n",
    "Deleting feature no. 194 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 263 of importance score -0.0018823529411764017\n",
    "Deleting feature no. 266 of importance score -0.0014117647058824012\n",
    "Deleting feature no. 103 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 162 of importance score -0.003529411764705914\n",
    "Deleting feature no. 103 of importance score -0.0021176470588235574\n",
    "Deleting feature no. 73 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 23 of importance score -0.0016470588235293793\n",
    "Deleting feature no. 162 of importance score -0.0021176470588234685\n",
    "Deleting feature no. 31 of importance score -0.001882352941176424\n",
    "Deleting feature no. 218 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 224 of importance score -0.0030588235294118247\n",
    "Deleting feature no. 215 of importance score -0.004235294117646981\n",
    "Deleting feature no. 137 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 199 of importance score -0.002823529411764758\n",
    "Deleting feature no. 78 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 3 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 14 of importance score -0.0030588235294116916\n",
    "Deleting feature no. 34 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 86 of importance score -0.002823529411764647\n",
    "Deleting feature no. 208 of importance score -0.0011764705882352678\n",
    "Deleting feature no. 235 of importance score -0.0014117647058824012\n",
    "Deleting feature no. 143 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 200 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 191 of importance score -0.0030588235294117137\n",
    "Deleting feature no. 18 of importance score -0.001411764705882379\n",
    "Deleting feature no. 11 of importance score -0.001882352941176424\n",
    "Deleting feature no. 150 of importance score -0.0030588235294117805\n",
    "Deleting feature no. 171 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 83 of importance score -0.002823529411764647\n",
    "Deleting feature no. 74 of importance score -0.003294117647058803\n",
    "Deleting feature no. 53 of importance score -0.0030588235294117805\n",
    "Deleting feature no. 34 of importance score -0.00235294117647058\n",
    "Deleting feature no. 135 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 171 of importance score -0.0030588235294118026\n",
    "Deleting feature no. 127 of importance score -0.0030588235294116916\n",
    "Deleting feature no. 18 of importance score -0.002823529411764736\n",
    "Deleting feature no. 204 of importance score -0.00258823529411758\n",
    "Deleting feature no. 195 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 161 of importance score -0.002588235294117647\n",
    "Deleting feature no. 177 of importance score -0.001647058823529468\n",
    "Deleting feature no. 229 of importance score -0.002588235294117691\n",
    "Deleting feature no. 106 of importance score -0.002117647058823513\n",
    "Deleting feature no. 228 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 153 of importance score -0.002823529411764669\n",
    "Deleting feature no. 32 of importance score -0.001882352941176424\n",
    "Deleting feature no. 220 of importance score -0.002117647058823513\n",
    "Deleting feature no. 121 of importance score -0.0016470588235293793\n",
    "Deleting feature no. 61 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 63 of importance score -0.002588235294117647\n",
    "Deleting feature no. 134 of importance score -0.001411764705882379\n",
    "Deleting feature no. 104 of importance score -0.001882352941176424\n",
    "Deleting feature no. 5 of importance score -0.002117647058823513\n",
    "Deleting feature no. 146 of importance score -0.001647058823529357\n",
    "Deleting feature no. 146 of importance score -0.001882352941176424\n",
    "Deleting feature no. 117 of importance score -0.001647058823529468\n",
    "Deleting feature no. 163 of importance score -0.001411764705882379\n",
    "Deleting feature no. 24 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 154 of importance score -0.0021176470588235574\n",
    "Deleting feature no. 37 of importance score -0.002823529411764758\n",
    "Deleting feature no. 72 of importance score -0.002823529411764758\n",
    "Deleting feature no. 49 of importance score -0.0023529411764706245\n",
    "Deleting feature no. 127 of importance score -0.0016470588235294459\n",
    "Deleting feature no. 40 of importance score -0.0011764705882353343\n",
    "Deleting feature no. 133 of importance score -0.0021176470588235795\n",
    "Deleting feature no. 66 of importance score -0.002823529411764647\n",
    "Deleting feature no. 50 of importance score -0.002588235294117602\n",
    "Deleting feature no. 205 of importance score -0.0011764705882352454\n",
    "Deleting feature no. 13 of importance score -0.002823529411764758\n",
    "Deleting feature no. 126 of importance score -0.004000000000000026\n",
    "Deleting feature no. 202 of importance score -0.00235294117647058\n",
    "Deleting feature no. 9 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 4 of importance score -0.0023529411764706245\n",
    "Deleting feature no. 86 of importance score -0.002117647058823513\n",
    "Deleting feature no. 55 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 3 of importance score -0.001882352941176424\n",
    "Deleting feature no. 103 of importance score -0.003294117647058825\n",
    "Deleting feature no. 158 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 187 of importance score -0.002117647058823513\n",
    "Deleting feature no. 153 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 136 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 104 of importance score -0.004235294117647026\n",
    "Deleting feature no. 151 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 112 of importance score -0.002823529411764625\n",
    "Deleting feature no. 149 of importance score -0.002117647058823513\n",
    "Deleting feature no. 134 of importance score -0.002588235294117713\n",
    "Deleting feature no. 143 of importance score -0.0035294117647058478\n",
    "Deleting feature no. 124 of importance score -0.0032941176470588475\n",
    "Deleting feature no. 106 of importance score -0.0016470588235294014\n",
    "Deleting feature no. 74 of importance score -0.004470588235294138\n",
    "Deleting feature no. 14 of importance score -0.0011764705882353343\n",
    "Deleting feature no. 8 of importance score -0.0023529411764706245\n",
    "Deleting feature no. 182 of importance score -0.0023529411764706687\n",
    "Deleting feature no. 172 of importance score -0.001882352941176424\n",
    "Deleting feature no. 58 of importance score -0.0023529411764706687\n",
    "Deleting feature no. 155 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 106 of importance score -0.002823529411764669\n",
    "Deleting feature no. 18 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 72 of importance score -0.0018823529411764683\n",
    "Deleting feature no. 120 of importance score -0.002588235294117647\n",
    "Deleting feature no. 168 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 117 of importance score -0.0023529411764705577\n",
    "Deleting feature no. 143 of importance score -0.00235294117647058\n",
    "Deleting feature no. 171 of importance score -0.0014117647058822902\n",
    "Deleting feature no. 60 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 124 of importance score -0.002588235294117647\n",
    "Deleting feature no. 46 of importance score -0.0016470588235293793\n",
    "Deleting feature no. 90 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 52 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 13 of importance score -0.0018823529411764906\n",
    "Deleting feature no. 142 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 34 of importance score -0.0030588235294116916\n",
    "Deleting feature no. 12 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 101 of importance score -0.001882352941176424\n",
    "Deleting feature no. 44 of importance score -0.003294117647058825\n",
    "Deleting feature no. 86 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 75 of importance score -0.002352941176470602\n",
    "Deleting feature no. 113 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 11 of importance score -0.0021176470588235574\n",
    "Deleting feature no. 35 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 128 of importance score -0.00235294117647058\n",
    "Deleting feature no. 13 of importance score -0.001882352941176535\n",
    "Deleting feature no. 141 of importance score -0.0014117647058822902\n",
    "Deleting feature no. 60 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 3 of importance score -0.0009411764705882675\n",
    "Deleting feature no. 13 of importance score -0.0011764705882353343\n",
    "Deleting feature no. 52 of importance score -0.0032941176470587586\n",
    "Deleting feature no. 66 of importance score -0.002823529411764669\n",
    "Deleting feature no. 18 of importance score -0.002117647058823513\n",
    "Deleting feature no. 101 of importance score -0.0030588235294118026\n",
    "Deleting feature no. 127 of importance score -0.002588235294117647\n",
    "Deleting feature no. 47 of importance score -0.0018823529411765127\n",
    "Deleting feature no. 42 of importance score -0.001647058823529468\n",
    "Deleting feature no. 115 of importance score -0.00352941176470587\n",
    "Deleting feature no. 104 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 119 of importance score -0.002823529411764625\n",
    "Deleting feature no. 128 of importance score -0.002823529411764736\n",
    "Deleting feature no. 65 of importance score -0.002588235294117647\n",
    "Deleting feature no. 51 of importance score -0.0014117647058823567\n",
    "Deleting feature no. 122 of importance score -0.004705882352941138\n",
    "Deleting feature no. 2 of importance score -0.0028235294117646913\n",
    "Deleting feature no. 31 of importance score -0.0037647058823529144\n",
    "Deleting feature no. 87 of importance score -0.0009411764705882008\n",
    "Deleting feature no. 45 of importance score -0.005647058823529472\n",
    "Deleting feature no. 89 of importance score -0.0009411764705882675\n",
    "Deleting feature no. 63 of importance score -0.003529411764705803\n",
    "Deleting feature no. 31 of importance score -0.002588235294117647\n",
    "Deleting feature no. 16 of importance score -0.002588235294117669\n",
    "Deleting feature no. 21 of importance score -0.0016470588235293793\n",
    "Deleting feature no. 121 of importance score -0.0016470588235294459\n",
    "Deleting feature no. 15 of importance score -0.0018823529411764017\n",
    "Deleting feature no. 12 of importance score -0.00235294117647058\n",
    "Deleting feature no. 23 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 79 of importance score -0.0030588235294117805\n",
    "Deleting feature no. 121 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 109 of importance score -0.001882352941176424\n",
    "Deleting feature no. 112 of importance score -0.001882352941176424\n",
    "Deleting feature no. 113 of importance score -0.0014117647058823567\n",
    "Deleting feature no. 14 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 62 of importance score -0.0021176470588235795\n",
    "Deleting feature no. 69 of importance score -0.0030588235294117584\n",
    "Deleting feature no. 31 of importance score -0.001882352941176424\n",
    "Deleting feature no. 38 of importance score -0.002588235294117647\n",
    "Deleting feature no. 105 of importance score -0.0014117647058823567\n",
    "Deleting feature no. 42 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 37 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 82 of importance score -0.0014117647058823346\n",
    "Deleting feature no. 5 of importance score -0.002588235294117669\n",
    "Deleting feature no. 61 of importance score -0.002588235294117669\n",
    "Deleting feature no. 12 of importance score -0.00023529411764704465\n",
    "Deleting feature no. 49 of importance score -0.0014117647058823567\n",
    "Deleting feature no. 25 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 61 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 98 of importance score -0.0016470588235294238\n",
    "Deleting feature no. 33 of importance score -0.00047058823529415593\n",
    "Deleting feature no. 76 of importance score -0.003529411764705914\n",
    "Deleting feature no. 37 of importance score -0.0016470588235294014\n",
    "Deleting feature no. 70 of importance score -0.0030588235294117584\n",
    "Deleting feature no. 38 of importance score -0.0037647058823530033\n",
    "Deleting feature no. 87 of importance score -0.001882352941176424\n",
    "Deleting feature no. 82 of importance score -0.002823529411764758\n",
    "Deleting feature no. 20 of importance score -0.003294117647058825\n",
    "Deleting feature no. 46 of importance score -0.004705882352941226\n",
    "Deleting feature no. 25 of importance score -0.0051764705882353605\n",
    "Deleting feature no. 2 of importance score -0.0030588235294118026\n",
    "Deleting feature no. 38 of importance score -0.004470588235294138\n",
    "Deleting feature no. 58 of importance score -0.004000000000000026\n",
    "Deleting feature no. 14 of importance score -0.001882352941176424\n",
    "Deleting feature no. 87 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 85 of importance score -0.003294117647058825\n",
    "Deleting feature no. 43 of importance score -0.00235294117647058\n",
    "Deleting feature no. 58 of importance score -0.0023529411764706466\n",
    "Deleting feature no. 63 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 8 of importance score -0.0030588235294117362\n",
    "Deleting feature no. 39 of importance score -0.002588235294117647\n",
    "Deleting feature no. 11 of importance score -0.001647058823529468\n",
    "Deleting feature no. 45 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 12 of importance score -0.003529411764705914\n",
    "Deleting feature no. 7 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 8 of importance score -0.0021176470588235795\n",
    "Deleting feature no. 29 of importance score -0.002823529411764758\n",
    "Deleting feature no. 55 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 30 of importance score -0.0004705882352940893\n",
    "Deleting feature no. 9 of importance score -0.003294117647058825\n",
    "Deleting feature no. 66 of importance score -0.0030588235294117805\n",
    "Deleting feature no. 50 of importance score -0.0016470588235294014\n",
    "Deleting feature no. 55 of importance score -0.00235294117647058\n",
    "Deleting feature no. 44 of importance score -0.002588235294117691\n",
    "Deleting feature no. 26 of importance score -0.001882352941176535\n",
    "Deleting feature no. 17 of importance score -0.0016470588235294459\n",
    "Deleting feature no. 16 of importance score -0.0009411764705882231\n",
    "Deleting feature no. 47 of importance score -0.002588235294117647\n",
    "Deleting feature no. 25 of importance score -0.0032941176470588475\n",
    "Deleting feature no. 12 of importance score -0.003529411764705892\n",
    "Deleting feature no. 11 of importance score -0.0018823529411764461\n",
    "Deleting feature no. 6 of importance score -0.004470588235294115\n",
    "Deleting feature no. 33 of importance score -0.002352941176470602\n",
    "Deleting feature no. 34 of importance score -0.0035294117647059363\n",
    "Deleting feature no. 56 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 25 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 53 of importance score -0.0021176470588234906\n",
    "Deleting feature no. 30 of importance score -0.0016470588235294459\n",
    "Deleting feature no. 13 of importance score -0.0011764705882352678\n",
    "Deleting feature no. 7 of importance score -0.0014117647058823125\n",
    "Deleting feature no. 11 of importance score -0.0021176470588236017\n",
    "Deleting feature no. 18 of importance score -0.0011764705882352454\n",
    "Deleting feature no. 2 of importance score -0.002823529411764736\n",
    "Deleting feature no. 18 of importance score -0.001647058823529357\n",
    "Deleting feature no. 12 of importance score -0.0009411764705882453\n",
    "Deleting feature no. 8 of importance score -0.0028235294117647802\n",
    "Deleting feature no. 29 of importance score -0.002117647058823513\n",
    "Deleting feature no. 19 of importance score -0.0014117647058824012\n",
    "Deleting feature no. 43 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 19 of importance score -0.0014117647058824012\n",
    "Deleting feature no. 24 of importance score -0.0021176470588235795\n",
    "Deleting feature no. 9 of importance score -0.0011764705882353122\n",
    "Deleting feature no. 23 of importance score -0.0032941176470588918\n",
    "Deleting feature no. 18 of importance score -0.0056470588235294494\n",
    "Deleting feature no. 30 of importance score -0.0007058823529411118\n",
    "Deleting feature no. 14 of importance score -0.002823529411764669\n",
    "Deleting feature no. 25 of importance score -0.0032941176470588475\n",
    "Deleting feature no. 12 of importance score -0.00023529411764702246\n",
    "Deleting feature no. 11 of importance score -0.0028235294117647134\n",
    "Deleting feature no. 22 of importance score -0.0009411764705882453\n",
    "Deleting feature no. 28 of importance score -0.0030588235294117137\n",
    "Deleting feature no. 10 of importance score -0.002117647058823513\n",
    "Deleting feature no. 7 of importance score -0.0021176470588235574\n",
    "Deleting feature no. 27 of importance score -0.004705882352941204\n",
    "Deleting feature no. 20 of importance score -0.0016470588235294459\n",
    "Deleting feature no. 24 of importance score 0.00047058823529406714\n",
    "Deleting feature no. 22 of importance score -0.00023529411764706688\n",
    "Deleting feature no. 8 of importance score -0.0007058823529411562\n",
    "Deleting feature no. 6 of importance score -0.0009411764705882453\n",
    "Deleting feature no. 8 of importance score 0.0014117647058823567\n",
    "Deleting feature no. 3 of importance score -0.0021176470588235353\n",
    "Deleting feature no. 13 of importance score -0.0007058823529411562\n",
    "Deleting feature no. 11 of importance score -0.0011764705882353122\n",
    "Deleting feature no. 3 of importance score -0.0025882352941176247\n",
    "Deleting feature no. 3 of importance score 0.0009411764705881786\n",
    "Deleting feature no. 9 of importance score -0.0009411764705882453\n",
    "Deleting feature no. 14 of importance score -0.0040000000000000036\n",
    "Deleting feature no. 10 of importance score 0.0014117647058823567\n",
    "Deleting feature no. 6 of importance score 0.003529411764705892\n",
    "Deleting feature no. 6 of importance score 0.0011764705882352678\n",
    "Deleting feature no. 4 of importance score 0.0018823529411764461\n",
    "Deleting feature no. 4 of importance score 0.0028235294117647134\n",
    "Deleting feature no. 2 of importance score 0.00117647058823529\n",
    "Deleting feature no. 5 of importance score 0.00352941176470587\n",
    "Deleting feature no. 4 of importance score 0.0051764705882353605\n",
    "Deleting feature no. 2 of importance score 0.008941176470588253\n",
    "Deleting feature no. 2 of importance score 0.010823529411764742\n",
    "Deleting feature no. 1 of importance score 0.022823529411764687\n",
    "Deleting feature no. 0 of importance score 0.04070588235294115\n",
    "Deleting feature no. 0 of importance score 0.07670588235294114\n",
    "Deleting feature no. 1 of importance score 0.08211764705882355\n",
    "Deleting feature no. 0 of importance score 0.08094117647058828\n",
    "\"\"\"\n",
    "\n",
    "selected_features = get_feature_args_from_str(log_text, drop_limit=494)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "f4052e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   6, 115, 283, 351, 402, 414, 425, 462])"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_args_from_str(log_text, drop_limit=490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a5ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "a6cd394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = X_train.copy(deep=True).iloc[:, selected_features]\n",
    "X_test_selected = X_test.copy(deep=True).iloc[:, selected_features]\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X_train_selected,label=y_train)\n",
    "params = {'objective':'binary:logistic',\n",
    "          'eval_metric':'logloss',\n",
    "          'eta':0.1,\n",
    "          'max_depth' : 8,\n",
    "          'device' : 'cuda',\n",
    "          'colsample_bytree':0.8,\n",
    "          'verbosity' : 1,\n",
    "          'alpha' : 7,}\n",
    "\n",
    "\n",
    "cvboosters = []\n",
    "xgb_cv = xgb.cv(\n",
    "    dtrain=data_dmatrix,\n",
    "    params=params,\n",
    "    nfold=5,\n",
    "    metrics = 'logloss',\n",
    "    seed=42,\n",
    "    num_boost_round = 300,\n",
    "    custom_metric = xgb_euros_gained,\n",
    "    early_stopping_rounds=100,\n",
    "    stratified = True,\n",
    "    callbacks=[SaveBestModel(cvboosters), ],\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "6f46ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "      <th>train-Euros_gained-mean</th>\n",
       "      <th>train-Euros_gained-std</th>\n",
       "      <th>test-Euros_gained-mean</th>\n",
       "      <th>test-Euros_gained-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.673799</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.674892</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>-6479.411817</td>\n",
       "      <td>88.529007</td>\n",
       "      <td>-6352.941211</td>\n",
       "      <td>283.331713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.657935</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.660421</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-6514.705859</td>\n",
       "      <td>119.471159</td>\n",
       "      <td>-6482.352832</td>\n",
       "      <td>353.724654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.644744</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.648497</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>-6561.764648</td>\n",
       "      <td>107.423356</td>\n",
       "      <td>-6494.117578</td>\n",
       "      <td>389.481074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.633896</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.639543</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>-6723.529394</td>\n",
       "      <td>79.683724</td>\n",
       "      <td>-6388.235449</td>\n",
       "      <td>209.133910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.624320</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.630936</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-6708.823437</td>\n",
       "      <td>59.843954</td>\n",
       "      <td>-6364.705957</td>\n",
       "      <td>215.007737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.616631</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.624038</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>-6691.176563</td>\n",
       "      <td>53.429105</td>\n",
       "      <td>-6388.235254</td>\n",
       "      <td>156.076457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.609959</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.618481</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>-6708.823535</td>\n",
       "      <td>49.565646</td>\n",
       "      <td>-6447.058691</td>\n",
       "      <td>184.522185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.604361</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.613936</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>-6685.294141</td>\n",
       "      <td>59.263109</td>\n",
       "      <td>-6423.529297</td>\n",
       "      <td>172.102838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.598819</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.609990</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>-6764.705859</td>\n",
       "      <td>108.065741</td>\n",
       "      <td>-6529.411719</td>\n",
       "      <td>178.420661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.594645</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>-6823.529297</td>\n",
       "      <td>92.542017</td>\n",
       "      <td>-6517.647070</td>\n",
       "      <td>172.102911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.590661</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.603801</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>-6870.588184</td>\n",
       "      <td>133.944767</td>\n",
       "      <td>-6494.117578</td>\n",
       "      <td>191.876549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.586392</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.601068</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>-7000.000000</td>\n",
       "      <td>123.038304</td>\n",
       "      <td>-6482.352832</td>\n",
       "      <td>155.187090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.582850</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.599399</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>-7064.705859</td>\n",
       "      <td>90.462112</td>\n",
       "      <td>-6494.117578</td>\n",
       "      <td>160.449251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.580430</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.597866</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>-7079.411817</td>\n",
       "      <td>78.148881</td>\n",
       "      <td>-6458.823633</td>\n",
       "      <td>218.202718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.578129</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.596439</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>-7058.823437</td>\n",
       "      <td>66.421117</td>\n",
       "      <td>-6505.882324</td>\n",
       "      <td>176.862259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.576229</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>-7061.764649</td>\n",
       "      <td>72.283508</td>\n",
       "      <td>-6517.646973</td>\n",
       "      <td>163.863398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.574069</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.594621</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>-7132.353027</td>\n",
       "      <td>87.249544</td>\n",
       "      <td>-6482.352930</td>\n",
       "      <td>150.661781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.571381</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.593615</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>-7214.705957</td>\n",
       "      <td>62.806347</td>\n",
       "      <td>-6458.823535</td>\n",
       "      <td>172.102691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.569161</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.593043</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>-7199.999902</td>\n",
       "      <td>52.283368</td>\n",
       "      <td>-6458.823438</td>\n",
       "      <td>183.770634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.567465</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.592591</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>-7226.470508</td>\n",
       "      <td>68.725290</td>\n",
       "      <td>-6447.058691</td>\n",
       "      <td>176.862337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.565226</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.591780</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>-7258.823633</td>\n",
       "      <td>88.038985</td>\n",
       "      <td>-6529.411816</td>\n",
       "      <td>162.165196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0             0.673799           0.000529           0.674892   \n",
       "1             0.657935           0.000618           0.660421   \n",
       "2             0.644744           0.000690           0.648497   \n",
       "3             0.633896           0.000961           0.639543   \n",
       "4             0.624320           0.000964           0.630936   \n",
       "5             0.616631           0.001138           0.624038   \n",
       "6             0.609959           0.001352           0.618481   \n",
       "7             0.604361           0.001498           0.613936   \n",
       "8             0.598819           0.001944           0.609990   \n",
       "9             0.594645           0.001963           0.606667   \n",
       "10            0.590661           0.001857           0.603801   \n",
       "11            0.586392           0.001875           0.601068   \n",
       "12            0.582850           0.002123           0.599399   \n",
       "13            0.580430           0.002216           0.597866   \n",
       "14            0.578129           0.002240           0.596439   \n",
       "15            0.576229           0.002323           0.595420   \n",
       "16            0.574069           0.002368           0.594621   \n",
       "17            0.571381           0.002549           0.593615   \n",
       "18            0.569161           0.003119           0.593043   \n",
       "19            0.567465           0.002985           0.592591   \n",
       "20            0.565226           0.002917           0.591780   \n",
       "\n",
       "    test-logloss-std  train-Euros_gained-mean  train-Euros_gained-std  \\\n",
       "0           0.000552             -6479.411817               88.529007   \n",
       "1           0.001080             -6514.705859              119.471159   \n",
       "2           0.001470             -6561.764648              107.423356   \n",
       "3           0.001911             -6723.529394               79.683724   \n",
       "4           0.002359             -6708.823437               59.843954   \n",
       "5           0.002973             -6691.176563               53.429105   \n",
       "6           0.003601             -6708.823535               49.565646   \n",
       "7           0.003874             -6685.294141               59.263109   \n",
       "8           0.004065             -6764.705859              108.065741   \n",
       "9           0.004485             -6823.529297               92.542017   \n",
       "10          0.004822             -6870.588184              133.944767   \n",
       "11          0.005271             -7000.000000              123.038304   \n",
       "12          0.005366             -7064.705859               90.462112   \n",
       "13          0.005547             -7079.411817               78.148881   \n",
       "14          0.005823             -7058.823437               66.421117   \n",
       "15          0.006028             -7061.764649               72.283508   \n",
       "16          0.006229             -7132.353027               87.249544   \n",
       "17          0.006579             -7214.705957               62.806347   \n",
       "18          0.006725             -7199.999902               52.283368   \n",
       "19          0.006864             -7226.470508               68.725290   \n",
       "20          0.007206             -7258.823633               88.038985   \n",
       "\n",
       "    test-Euros_gained-mean  test-Euros_gained-std  \n",
       "0             -6352.941211             283.331713  \n",
       "1             -6482.352832             353.724654  \n",
       "2             -6494.117578             389.481074  \n",
       "3             -6388.235449             209.133910  \n",
       "4             -6364.705957             215.007737  \n",
       "5             -6388.235254             156.076457  \n",
       "6             -6447.058691             184.522185  \n",
       "7             -6423.529297             172.102838  \n",
       "8             -6529.411719             178.420661  \n",
       "9             -6517.647070             172.102911  \n",
       "10            -6494.117578             191.876549  \n",
       "11            -6482.352832             155.187090  \n",
       "12            -6494.117578             160.449251  \n",
       "13            -6458.823633             218.202718  \n",
       "14            -6505.882324             176.862259  \n",
       "15            -6517.646973             163.863398  \n",
       "16            -6482.352930             150.661781  \n",
       "17            -6458.823535             172.102691  \n",
       "18            -6458.823438             183.770634  \n",
       "19            -6447.058691             176.862337  \n",
       "20            -6529.411816             162.165196  "
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e733d13",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "ddbbff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Format: (metric_name, value, is_higher_better)\n",
    "def catboost_euros_gained(y_true, y_pred_proba, feature_num):\n",
    "    n_selected = int(1 / 5 * len(y_true))\n",
    "    top_k_indices = np.argsort(y_pred_proba[:, 1])[-n_selected:]\n",
    "    true_positives = y_true.iloc[top_k_indices].sum()\n",
    "\n",
    "    max_reward = n_selected * 10\n",
    "    reward = true_positives * 10 * 10000 / max_reward\n",
    "    cost = feature_num * 200\n",
    "    euros = reward - cost\n",
    "\n",
    "    return \"Euros_gained\", euros, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "83d48151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 7551.9375 Total: 11882.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold [0/5]\n",
      "bestTest = 0.6049666982\n",
      "bestIteration = 166\n",
      "Training on fold [1/5]\n",
      "bestTest = 0.6051198957\n",
      "bestIteration = 203\n",
      "Training on fold [2/5]\n",
      "bestTest = 0.5848068776\n",
      "bestIteration = 213\n",
      "Training on fold [3/5]\n",
      "bestTest = 0.6056954212\n",
      "bestIteration = 176\n",
      "Training on fold [4/5]\n",
      "bestTest = 0.6022611333\n",
      "bestIteration = 208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_selected = X_train.copy(deep=True).iloc[:, selected_features]\n",
    "X_test_selected = X_test.copy(deep=True).iloc[:, selected_features]\n",
    "\n",
    "\n",
    "train_pool = Pool(data=X_train_selected, label=y_train)\n",
    "cat_params = {\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'Logloss',\n",
    "    'learning_rate': 0.02,\n",
    "    'depth': 1,\n",
    "    'random_seed': 42,\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0',\n",
    "    'verbose': 0,\n",
    "    'reg_lambda' : 30,\n",
    "    'bagging_temperature' : 0\n",
    "}\n",
    "\n",
    "cv_data = cv(\n",
    "    pool=train_pool,\n",
    "    params=cat_params,\n",
    "    fold_count=5,\n",
    "    stratified=True,\n",
    "    early_stopping_rounds=10,\n",
    "    iterations=500,\n",
    "    partition_random_seed=42,\n",
    "    verbose=False,\n",
    "    as_pandas=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "1faf914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>test-Logloss-mean</th>\n",
       "      <th>test-Logloss-std</th>\n",
       "      <th>train-Logloss-mean</th>\n",
       "      <th>train-Logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.689642</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.689609</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.686267</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.686211</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.683082</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.682971</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.680024</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.677050</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.676850</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>0.600346</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.597846</td>\n",
       "      <td>0.002008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>212</td>\n",
       "      <td>0.600354</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.597844</td>\n",
       "      <td>0.002004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>213</td>\n",
       "      <td>0.600353</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.597844</td>\n",
       "      <td>0.002004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>214</td>\n",
       "      <td>0.600356</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.597843</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>215</td>\n",
       "      <td>0.600355</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.597843</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iterations  test-Logloss-mean  test-Logloss-std  train-Logloss-mean  \\\n",
       "0             0           0.689642          0.000164            0.689609   \n",
       "1             1           0.686267          0.000313            0.686211   \n",
       "2             2           0.683082          0.000478            0.682971   \n",
       "3             3           0.680024          0.000580            0.679856   \n",
       "4             4           0.677050          0.000687            0.676850   \n",
       "..          ...                ...               ...                 ...   \n",
       "211         211           0.600346          0.008299            0.597846   \n",
       "212         212           0.600354          0.008283            0.597844   \n",
       "213         213           0.600353          0.008283            0.597844   \n",
       "214         214           0.600356          0.008284            0.597843   \n",
       "215         215           0.600355          0.008284            0.597843   \n",
       "\n",
       "     train-Logloss-std  \n",
       "0             0.000093  \n",
       "1             0.000177  \n",
       "2             0.000255  \n",
       "3             0.000335  \n",
       "4             0.000427  \n",
       "..                 ...  \n",
       "211           0.002008  \n",
       "212           0.002004  \n",
       "213           0.002004  \n",
       "214           0.002005  \n",
       "215           0.002005  \n",
       "\n",
       "[216 rows x 5 columns]"
      ]
     },
     "execution_count": 1041,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "id": "06a892b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 7666.75 Total: 11882.125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Euros_gained',\n",
       " 0    7600.0\n",
       " dtype: float64,\n",
       " True)"
      ]
     },
     "execution_count": 1073,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iteration = np.argmin(cv_data['test-Logloss-mean'])\n",
    "model = CatBoostClassifier(iterations=170, **cat_params)\n",
    "model.fit(train_pool)\n",
    "\n",
    "test_pool = Pool(data=X_test_selected, label=y_test)\n",
    "catboost_euros_gained(y_test, model.predict_proba(test_pool), len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "id": "6ba475df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7213333333333334\n",
      "0.7138964577656676\n",
      "0.7148703956343793\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_test,  model.predict(test_pool)))\n",
    "print(metrics.precision_score(y_test, model.predict(test_pool)))\n",
    "print(metrics.f1_score(y_test, model.predict(test_pool)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b43dbf",
   "metadata": {},
   "source": [
    "# Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "d48eefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpuscian/Envs/AML/lib64/python3.13/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "mi_scores = feature_selection.mutual_info_classif(X_train, y_train, discrete_features='auto', random_state=42)\n",
    "\n",
    "# Create a DataFrame to easily inspect scores\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'score': mi_scores\n",
    "}).sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "5b242fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features selected by mutual_info_classif:\n",
      "   feature     score\n",
      "2        2  0.090186\n"
     ]
    }
   ],
   "source": [
    "top_k = 1\n",
    "selected_features = mi_df.head(top_k)['feature'].tolist()\n",
    "#selected_features = [2]\n",
    "print(\"Top features selected by mutual_info_classif:\")\n",
    "print(mi_df.head(top_k))\n",
    "\n",
    "X_train_selected = X_train.copy(deep=True).iloc[:, selected_features]\n",
    "X_test_selected = X_test.copy(deep=True).iloc[:, selected_features]\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X_train_selected,label=y_train)\n",
    "params = {'objective':'binary:logistic',\n",
    "          'eta':0.03,\n",
    "          'max_depth' : 1,\n",
    "          'device' : 'cuda',\n",
    "          'subsample':0.9,\n",
    "          'verbosity' : 1,\n",
    "          'alpha' : 8,\n",
    "          'lambda' : 400}\n",
    "\n",
    "\n",
    "cvboosters = []\n",
    "xgb_cv = xgb.cv(\n",
    "    dtrain=data_dmatrix,\n",
    "    params=params,\n",
    "    nfold=5,\n",
    "    metrics = 'logloss',\n",
    "    seed=42,\n",
    "    num_boost_round = 146,\n",
    "    custom_metric = xgb_euros_gained,\n",
    "    #early_stopping_rounds=100,\n",
    "    stratified = True,\n",
    "    callbacks=[SaveBestModel(cvboosters), ],\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "afc4a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7458.823535\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "      <th>train-Euros_gained-mean</th>\n",
       "      <th>train-Euros_gained-std</th>\n",
       "      <th>test-Euros_gained-mean</th>\n",
       "      <th>test-Euros_gained-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683091</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.683260</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>-7208.823535</td>\n",
       "      <td>79.683522</td>\n",
       "      <td>-7117.647070</td>\n",
       "      <td>316.992884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674474</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.674703</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-7200.000000</td>\n",
       "      <td>72.879381</td>\n",
       "      <td>-7152.941211</td>\n",
       "      <td>302.240849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666764</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.667089</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>-7200.000000</td>\n",
       "      <td>72.879381</td>\n",
       "      <td>-7152.941211</td>\n",
       "      <td>302.240849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.659915</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.660437</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>-7200.000000</td>\n",
       "      <td>72.879381</td>\n",
       "      <td>-7141.176562</td>\n",
       "      <td>318.734526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.653896</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.654630</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>-7200.000000</td>\n",
       "      <td>72.879381</td>\n",
       "      <td>-7141.176562</td>\n",
       "      <td>318.734526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5.891640</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>6.003245</td>\n",
       "      <td>0.483047</td>\n",
       "      <td>-7267.646973</td>\n",
       "      <td>128.000361</td>\n",
       "      <td>-7411.764648</td>\n",
       "      <td>256.675557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.891928</td>\n",
       "      <td>0.266229</td>\n",
       "      <td>6.003500</td>\n",
       "      <td>0.483078</td>\n",
       "      <td>-7267.646973</td>\n",
       "      <td>128.000361</td>\n",
       "      <td>-7411.764648</td>\n",
       "      <td>256.675557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5.891536</td>\n",
       "      <td>0.266365</td>\n",
       "      <td>6.003149</td>\n",
       "      <td>0.483119</td>\n",
       "      <td>-7267.646973</td>\n",
       "      <td>128.000361</td>\n",
       "      <td>-7411.764648</td>\n",
       "      <td>256.675557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>5.891370</td>\n",
       "      <td>0.266688</td>\n",
       "      <td>6.003117</td>\n",
       "      <td>0.482908</td>\n",
       "      <td>-7264.705859</td>\n",
       "      <td>132.385558</td>\n",
       "      <td>-7458.823535</td>\n",
       "      <td>349.789958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5.891151</td>\n",
       "      <td>0.267002</td>\n",
       "      <td>6.003094</td>\n",
       "      <td>0.482573</td>\n",
       "      <td>-7264.705859</td>\n",
       "      <td>132.385558</td>\n",
       "      <td>-7458.823535</td>\n",
       "      <td>349.789958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.683091           0.000318           0.683260   \n",
       "1              0.674474           0.000539           0.674703   \n",
       "2              0.666764           0.000835           0.667089   \n",
       "3              0.659915           0.001151           0.660437   \n",
       "4              0.653896           0.001499           0.654630   \n",
       "..                  ...                ...                ...   \n",
       "141            5.891640           0.266482           6.003245   \n",
       "142            5.891928           0.266229           6.003500   \n",
       "143            5.891536           0.266365           6.003149   \n",
       "144            5.891370           0.266688           6.003117   \n",
       "145            5.891151           0.267002           6.003094   \n",
       "\n",
       "     test-logloss-std  train-Euros_gained-mean  train-Euros_gained-std  \\\n",
       "0            0.000579             -7208.823535               79.683522   \n",
       "1            0.001135             -7200.000000               72.879381   \n",
       "2            0.001757             -7200.000000               72.879381   \n",
       "3            0.002380             -7200.000000               72.879381   \n",
       "4            0.002991             -7200.000000               72.879381   \n",
       "..                ...                      ...                     ...   \n",
       "141          0.483047             -7267.646973              128.000361   \n",
       "142          0.483078             -7267.646973              128.000361   \n",
       "143          0.483119             -7267.646973              128.000361   \n",
       "144          0.482908             -7264.705859              132.385558   \n",
       "145          0.482573             -7264.705859              132.385558   \n",
       "\n",
       "     test-Euros_gained-mean  test-Euros_gained-std  \n",
       "0              -7117.647070             316.992884  \n",
       "1              -7152.941211             302.240849  \n",
       "2              -7152.941211             302.240849  \n",
       "3              -7141.176562             318.734526  \n",
       "4              -7141.176562             318.734526  \n",
       "..                      ...                    ...  \n",
       "141            -7411.764648             256.675557  \n",
       "142            -7411.764648             256.675557  \n",
       "143            -7411.764648             256.675557  \n",
       "144            -7458.823535             349.789958  \n",
       "145            -7458.823535             349.789958  \n",
       "\n",
       "[146 rows x 8 columns]"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xgb_cv.iloc[:, 6].min())\n",
    "xgb_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "04c19401",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_selected = pd.concat([X_train_selected, X_test_selected])\n",
    "y_full = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "id": "bd63e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = pd.read_csv('/home/mpuscian/Desktop/repozytoria/MINI_projects/AML_Project_2/x_test.txt', sep=' ', header=None).iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "a6898f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clf.predict_proba(X_test_final)[:, 1]\n",
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "74370969",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/mpuscian/Desktop/repozytoria/MINI_projects/AML_Project_2/305995_obs2.txt', clf.predict_proba(X_test_final)[:, 1].argsort()[-1000:], delimiter='\\n', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "1084eb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Euros_gained', np.float64(-7400.0))"
      ]
     },
     "execution_count": 1186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_selected = X_train.copy(deep=True).iloc[:, selected_features]\n",
    "X_test_selected = X_test.copy(deep=True).iloc[:, selected_features]\n",
    "\n",
    "params = {'objective':'binary:logistic',\n",
    "          'eta':0.03,\n",
    "          'max_depth' : 1,\n",
    "          'device' : 'cuda',\n",
    "          'subsample':0.9,\n",
    "          'verbosity' : 1,\n",
    "          'alpha' : 8,\n",
    "          'lambda' : 400}\n",
    "\n",
    "clf = xgb.XGBClassifier(**params, n_estimators = 147)\n",
    "clf.fit(X_full_selected, y_full)\n",
    "\n",
    "euros_gained(y_test.to_numpy(), clf.predict_proba(X_test_selected), len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ebda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "55f0570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266666666666667\n",
      "0.7293447293447294\n",
      "0.7140864714086471\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_test,  clf.predict(X_test_selected)))\n",
    "print(metrics.precision_score(y_test, clf.predict(X_test_selected)))\n",
    "print(metrics.f1_score(y_test, clf.predict(X_test_selected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cfd87",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "e2eb4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_proba(catboost_model, xgb_model, pool, X_selected, method='max'):\n",
    "    proba_cat = catboost_model.predict_proba(pool)\n",
    "    proba_xgb = xgb_model.predict_proba(X_selected)\n",
    "\n",
    "    assert proba_cat.shape == proba_xgb.shape, \"Mismatch in prediction shape between models\"\n",
    "\n",
    "    if method == 'max':\n",
    "        proba_cat_max = np.max(proba_cat, axis=1)\n",
    "        proba_xgb_max = np.max(proba_xgb, axis=1)\n",
    "\n",
    "        use_cat = proba_cat_max > proba_xgb_max\n",
    "        ensemble_proba = np.where(use_cat[:, None], proba_cat, proba_xgb)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "    \n",
    "    return ensemble_proba\n",
    "\n",
    "def get_ensemble_prediction(catboost_model, xgb_model, pool, X_selected, method='max'):\n",
    "    proba = get_ensemble_proba(catboost_model, xgb_model, pool, X_selected, method=method)\n",
    "    return np.argmax(proba, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93881b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Euros_gained', np.float64(-7533.333333333333))"
      ]
     },
     "execution_count": 1147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euros_gained(y_test.to_numpy(), get_ensemble_proba(model, clf, test_pool, X_test_selected, method='max'), len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "f13cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Euros_gained', np.float64(-7466.666666666667))"
      ]
     },
     "execution_count": 1148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euros_gained(y_test.to_numpy(), clf.predict_proba(X_test_selected), len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "65622901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Euros_gained', np.float64(-7600.0))"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euros_gained(y_test.to_numpy(), model.predict_proba(test_pool), len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "7fc661da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266666666666667\n",
      "0.7293447293447294\n",
      "0.7140864714086471\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,  get_ensemble_prediction(model, clf, test_pool, X_test_selected, method='max')))\n",
    "print(metrics.precision_score(y_test, get_ensemble_prediction(model, clf, test_pool, X_test_selected, method='max')))\n",
    "print(metrics.f1_score(y_test, get_ensemble_prediction(model, clf, test_pool, X_test_selected, method='max')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlkernel",
   "language": "python",
   "name": "amlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
